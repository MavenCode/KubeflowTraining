{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on CHURN Dataset using Tensorflow Operator\n",
    "\n",
    "## Prerequisites\n",
    "Before we proceed, let's check that we're using the right image, that is, [TensorFlow](https://www.tensorflow.org/api_docs/) is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 list | grep tensorflow \n",
    "! pip3 install --user tensorflow==2.4.0\n",
    "! pip3 install --user ipywidgets nbconvert\n",
    "!python -m pip install --user --upgrade pip\n",
    "!pip3 install pandas scikit-learn keras tensorflow-datasets --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"tfjobchurn.py\"\n",
    "KUBERNETES_FILE = \"tfjob-churn.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to capture output from a cell with [`%%capture`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture) that usually looks like `some-resource created`.\n",
    "To that end, let's define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Load and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd\n",
    "\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/AdeloreSimiloluwa/Artificial-Neural-Network/master/data/Churn_Modelling.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Train the Model in the Notebook\n",
    "We want to train the model in a distributed fashion, we put all the code in a single cell.\n",
    "That way we can save the file and include it in a container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "trainer_code"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Standardization - feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# data encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "  data = pd.read_csv(\"https://raw.githubusercontent.com/AdeloreSimiloluwa/Artificial-Neural-Network/master/data/Churn_Modelling.csv\")\n",
    "\n",
    "  #preprocessing\n",
    "  X = data.iloc[:, 3:-1]\n",
    "  y = data.iloc[:,-1:]\n",
    "\n",
    "  # encoding country\n",
    "  encoder_X_1= LabelEncoder()\n",
    "  X.iloc[:,1] = encoder_X_1.fit_transform(X.iloc[:,1])\n",
    "\n",
    "  # encoding gender\n",
    "  encoder_X_2= LabelEncoder()\n",
    "  X.iloc[:,2] = encoder_X_2.fit_transform(X.iloc[:,2])\n",
    "\n",
    "  # we would also use the dummy variable because they are norminal variables\n",
    "  dummy = pd.get_dummies(X[\"Geography\"], prefix = ['Geography'],drop_first=True)\n",
    "  X=pd.concat([X,dummy], axis = 1)\n",
    "  X=X.drop(columns = ['Geography'], axis = 1)\n",
    "    \n",
    "  # split the data\n",
    "  X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 10)\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "  train = train_dataset.cache().shuffle(2000).repeat()\n",
    "  return train, test_dataset\n",
    "\n",
    "\n",
    "def model(args):\n",
    "  model = models.Sequential()\n",
    "  model.add(Dense(units =9, activation='relu', input_dim=11))\n",
    "  model.add(Dense(units =9, activation='relu'))\n",
    "  model.add(Dense(units =1, activation='sigmoid'))\n",
    "\n",
    "  model.summary()\n",
    "  opt = args.optimizer\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
    "  return model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "  # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "  # layers on each device across all workers\n",
    "  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "      communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "  logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
    "  BATCH_SIZE_PER_REPLICA = args.batch_size\n",
    "  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "  # Datasets need to be created after instantiation of `MultiWorkerMirroredStrategy`\n",
    "  train_dataset, test_dataset = make_datasets_unbatched()\n",
    "  train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "  test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "  # See: https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions\n",
    "  options = tf.data.Options()\n",
    "  options.experimental_distribute.auto_shard_policy = \\\n",
    "        tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "  train_datasets_sharded  = train_dataset.with_options(options)\n",
    "  test_dataset_sharded = test_dataset.with_options(options)\n",
    "\n",
    "  with strategy.scope():\n",
    "    # Model building/compiling need to be within `strategy.scope()`.\n",
    "    multi_worker_model = model(args)\n",
    "\n",
    "  # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "  # number of steps per epoch. \n",
    "  multi_worker_model.fit(train_datasets_sharded,\n",
    "                         epochs=50,\n",
    "                         steps_per_epoch=30)\n",
    "  \n",
    "  eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
    "                                                    verbose=0, steps=10)\n",
    "\n",
    "  # Log metrics for Katib\n",
    "  logging.info(\"loss={:.4f}\".format(eval_loss))\n",
    "  logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--batch_size\",\n",
    "                      type=int,\n",
    "                      default=32,\n",
    "                      metavar=\"N\",\n",
    "                      help=\"Batch size for training (default: 128)\")\n",
    "  parser.add_argument(\"--learning_rate\", \n",
    "                      type=float,  \n",
    "                      default=0.1,\n",
    "                      metavar=\"N\",\n",
    "                      help='Initial learning rate')\n",
    "  parser.add_argument(\"--optimizer\", \n",
    "                      type=str, \n",
    "                      default='adam',\n",
    "                      metavar=\"N\",\n",
    "                      help='optimizer')\n",
    "\n",
    "  parsed_args, _ = parser.parse_known_args()\n",
    "  main(parsed_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That saves the file as defined by `TRAINER_FILE` but it does not run it.\n",
    "\n",
    "Let's see if our code is correct by running it from within our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run $TRAINER_FILE --optimizer 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "trainer_dockerfile"
    ]
   },
   "source": [
    "## How to Create a Docker Image Manually\n",
    "\n",
    "\n",
    "The Dockerfile looks as follows:\n",
    "\n",
    "```\n",
    "  \n",
    "FROM tensorflow/tensorflow:2.4.0\n",
    "RUN pip install tensorflow_datasets pandas scikit-learn keras\n",
    "COPY tfjobchurn.py /\n",
    "ENTRYPOINT [\"python\", \"/tfjobchurn.py\", \"--batch_size\", \"64\", \"--learning_rate\", \"0.1\", \"--optimizer\", \"adam\"]\n",
    "```\n",
    "\n",
    "\n",
    "Then it's easy to push images to your container registry:\n",
    "\n",
    "```bash\n",
    "docker build -t <docker_image_name_with_tag> .\n",
    "docker push <docker_image_name_with_tag>\n",
    "```\n",
    "\n",
    "The image is available as `mavencodev/tf_jobchurn:1.0` in case you want to skip it for now.\n",
    "\n",
    "## How to Create a Distributed `TFJob`\n",
    "For large training jobs, we wish to run our trainer in a distributed mode.\n",
    "Once the notebook server cluster can access the Docker image from the registry, we can launch a distributed PyTorch job.\n",
    "\n",
    "The specification for a distributed `TFJob` is defined using YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: \"TFJob\"\n",
    "metadata:\n",
    "  name: \"churn\"\n",
    "  namespace: demo01 # your-user-namespace\n",
    "spec:\n",
    "  cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: 2\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: tensorflow\n",
    "            # modify this property if you would like to use a custom image\n",
    "            image: mavencodev/tf_jobchurn:1.0\n",
    "            command:\n",
    "                - \"python\"\n",
    "                - \"/tfjobchurn.py\"\n",
    "                - \"--batch_size=64\"\n",
    "                - \"--learning_rate=0.1\"\n",
    "                - \"--optimizer=adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy the distributed training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture tf_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_JOB = get_resource(tf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the job status, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl describe $TF_JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the created pods matching the specified number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods -l job-name=churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of issues, it may be helpful to see the last ten events within the cluster:\n",
    "\n",
    "```bash\n",
    "! kubectl get events --sort-by='.lastTimestamp' | tail\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stream logs from the worker-0 pod to check the training progress, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl logs -f churn-worker-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete $TF_JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the check to see if the pod is still up and running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n demo01 logs -f churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
