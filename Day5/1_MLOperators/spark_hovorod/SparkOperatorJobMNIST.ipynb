{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST Dataset using Spark Operator and Horovod\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"spark_mnist.py\"\n",
    "KUBERNETES_FILE = \"sparkapp-mnist.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to capture output from a cell with [`%%capture`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture) that usually looks like `some-resource created`.\n",
    "To that end, let's define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created (e.g. already exists)\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Train the Model in the Notebook\n",
    "Since we ultimately want to train the model in a distributed fashion (potentially on GPUs), we put all the code in a single cell.\n",
    "That way we can save the file and include it in a container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "trainer_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import horovod.spark #https://github.com/horovod/horovod\n",
    "import horovod.tensorflow.keras as hvd\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def get_dataset(rank=0, size=1):\n",
    "    with np.load('datasets/mnist.npz', allow_pickle=True) as f:\n",
    "        x_train = f['x_train'][rank::size]\n",
    "        y_train = f['y_train'][rank::size]\n",
    "        x_test = f['x_test'][rank::size]\n",
    "        y_test = f['y_test'][rank::size]\n",
    "        x_train, x_test = x_train / 255.0, x_test / 255.0 # Normalize RGB values to [0, 1]\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def deserialize(model_bytes):\n",
    "    import horovod.tensorflow.keras as hvd\n",
    "    import h5py\n",
    "    import io\n",
    "    bio = io.BytesIO(model_bytes)\n",
    "    with h5py.File(bio, 'a') as f:\n",
    "        return hvd.load_model(f)\n",
    "\n",
    "\n",
    "def predict_number(model, x_test, image_index):\n",
    "    pred = model.predict(x_test[image_index:image_index + 1])\n",
    "    print(f\"Model prediction for index {image_index}: {pred.argmax()}\")\n",
    "\n",
    "\n",
    "def train_hvd(learning_rate, batch_size, epochs):\n",
    "    # 1 - Initialize Horovod\n",
    "    hvd.init()\n",
    "\n",
    "    # 2 - Pin GPUs\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n",
    "    model = get_model()\n",
    "\n",
    "    # 3 - Wrap optimizer\n",
    "    optimizer = hvd.DistributedOptimizer(\n",
    "        # 4- Scale learning rate\n",
    "        tf.optimizers.Adam(lr=learning_rate * hvd.size())\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',experimental_run_tf_function=False, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        # 5 - Broadcast initial variables\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "        hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=3, verbose=1),\n",
    "    ]\n",
    "\n",
    "    # 6 - Save checkpoints\n",
    "    ckpt_dir = tempfile.mkdtemp()\n",
    "    ckpt_file = os.path.join(ckpt_dir, 'checkpoint.h5')\n",
    "    if hvd.rank() == 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ModelCheckpoint(ckpt_file, monitor='accuracy', mode='max',\n",
    "                                               save_best_only=True))\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks,\n",
    "                        epochs=epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        with open(ckpt_file, 'rb') as f:\n",
    "            return history.history, f.read()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Horovod-on-Spark MNIST Training Job\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=int,\n",
    "        default=0.001,\n",
    "        metavar=\"N\",\n",
    "        help=\"Learning rate (default: 0.001)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"Batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        metavar=\"N\",\n",
    "        help=\"Number of epochs to train (default: 5)\",\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    spark = SparkSession.builder.appName(\"HorovodOnSpark\").getOrCreate()\n",
    "\n",
    "    image_index = 100\n",
    "    (x_train, y_train), (x_test, y_test) = get_dataset()\n",
    "    \n",
    "    print(f\"Expected prediction for index {image_index}: {y_test[image_index]}\")\n",
    "    \n",
    "    # Train model with Horovod on Spark\n",
    "    model_bytes = horovod.spark.run(train_hvd, args=(args.learning_rate,\n",
    "                                                     args.batch_size,\n",
    "                                                     args.epochs))[0][1]\n",
    "\n",
    "    model = deserialize(model_bytes)\n",
    "    model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "    predict_number(model, x_test, image_index)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HOROVOD_JOB=spark_mnist.py\n"
     ]
    }
   ],
   "source": [
    "%env HOROVOD_JOB=$TRAINER_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the training job, let's first run it on Spark in a local mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: /bin/spark-submit: not found\n"
     ]
    }
   ],
   "source": [
    "! ${SPARK_HOME}/bin/spark-submit --master local[1] $HOROVOD_JOB --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "trainer_dockerfile"
    ]
   },
   "source": [
    "\n",
    "\n",
    "For those interested, the Dockerfile for this training can be built locally\n",
    "\n",
    "```\n",
    "FROM mavencodev/sparkjob:1.0\n",
    "ADD mnist.py /\n",
    "ADD datasets /datasets\n",
    "\n",
    "WORKDIR /\n",
    "```\n",
    "\n",
    "If GPU support is not needed, you can leave off the `-gpu` suffix from the image.\n",
    "`mnist.py` is the trainer code you have to download to your local machine.\n",
    "\n",
    "Then it's easy to push images to your container registry:\n",
    "\n",
    "```bash\n",
    "docker build -t <docker_image_name_with_tag> .\n",
    "docker push <docker_image_name_with_tag>\n",
    "```\n",
    "\n",
    "The image is available as `mavencodev/sparkjob:1.0` in case you want to skip it for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sparkapp-mnist.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"sparkoperator.k8s.io/v1beta2\"\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "  name: horovod-mnist-0\n",
    "spec:\n",
    "  type: Python\n",
    "  mode: cluster\n",
    "  pythonVersion: \"3\"\n",
    "  image: mavencodev/sparkjob:1.0\n",
    "  imagePullPolicy: Always  \n",
    "  mainApplicationFile: \"local:///mnist.py\"\n",
    "  sparkVersion: \"3.0.0\"\n",
    "  restartPolicy:\n",
    "    type: Never\n",
    "  arguments:\n",
    "    - --epochs\n",
    "    - \"10\"\n",
    "  driver:\n",
    "    env:\n",
    "    - name: PYTHONUNBUFFERED\n",
    "      value: \"1\"\n",
    "    cores: 1\n",
    "    memory: \"1G\"\n",
    "    labels:\n",
    "      version: 3.0.0\n",
    "      metrics-exposed: \"true\"  \n",
    "    annotations:\n",
    "      sidecar.istio.io/inject: \"false\"\n",
    "    serviceAccount: default-editor\n",
    "  executor:\n",
    "    cores: 1\n",
    "    instances: 5\n",
    "    memory: \"512m\"\n",
    "    labels:\n",
    "      version: 3.0.0\n",
    "      metrics-exposed: \"true\"  \n",
    "    annotations:\n",
    "      sidecar.istio.io/inject: \"false\"\n",
    "  monitoring:\n",
    "    exposeDriverMetrics: true\n",
    "    exposeExecutorMetrics: true\n",
    "    prometheus:\n",
    "      jmxExporterJar: \"/prometheus/jmx_prometheus_javaagent-0.11.0.jar\"\n",
    "      port: 8090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's deploy the distributed training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture hvd_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HVD_JOB = get_resource(hvd_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the pods are being created according to our specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   READY   STATUS      RESTARTS   AGE\n",
      "horovod-mnist-driver   0/1     Completed   0          15h\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods -l sparkoperator.k8s.io/app-name=horovod-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the model prediction (as before) by looking at the logs of the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction for index 100: 6\n"
     ]
    }
   ],
   "source": [
    "! kubectl logs horovod-mnist-driver | grep 'Model prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise we can see the status of the `horovod-mnist` `SparkApplication`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         horovod-mnist-0\n",
      "Namespace:    demo01\n",
      "Labels:       <none>\n",
      "Annotations:  <none>\n",
      "API Version:  sparkoperator.k8s.io/v1beta2\n",
      "Kind:         SparkApplication\n",
      "Metadata:\n",
      "  Creation Timestamp:  2021-03-17T12:32:24Z\n",
      "  Generation:          1\n",
      "  Managed Fields:\n",
      "    API Version:  sparkoperator.k8s.io/v1beta2\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:spec:\n",
      "        .:\n",
      "        f:arguments:\n",
      "        f:driver:\n",
      "          .:\n",
      "          f:annotations:\n",
      "            .:\n",
      "            f:sidecar.istio.io/inject:\n",
      "          f:cores:\n",
      "          f:env:\n",
      "          f:labels:\n",
      "            .:\n",
      "            f:metrics-exposed:\n",
      "            f:version:\n",
      "          f:memory:\n",
      "          f:serviceAccount:\n",
      "        f:executor:\n",
      "          .:\n",
      "          f:annotations:\n",
      "            .:\n",
      "            f:sidecar.istio.io/inject:\n",
      "          f:cores:\n",
      "          f:instances:\n",
      "          f:labels:\n",
      "            .:\n",
      "            f:metrics-exposed:\n",
      "            f:version:\n",
      "          f:memory:\n",
      "        f:image:\n",
      "        f:imagePullPolicy:\n",
      "        f:mainApplicationFile:\n",
      "        f:mode:\n",
      "        f:monitoring:\n",
      "          .:\n",
      "          f:exposeDriverMetrics:\n",
      "          f:exposeExecutorMetrics:\n",
      "          f:prometheus:\n",
      "            .:\n",
      "            f:jmxExporterJar:\n",
      "            f:port:\n",
      "        f:pythonVersion:\n",
      "        f:restartPolicy:\n",
      "          .:\n",
      "          f:type:\n",
      "        f:sparkVersion:\n",
      "        f:type:\n",
      "    Manager:      kubectl-create\n",
      "    Operation:    Update\n",
      "    Time:         2021-03-17T12:32:24Z\n",
      "    API Version:  sparkoperator.k8s.io/v1beta2\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:spec:\n",
      "        f:deps:\n",
      "      f:status:\n",
      "        .:\n",
      "        f:applicationState:\n",
      "          .:\n",
      "          f:state:\n",
      "        f:driverInfo:\n",
      "          .:\n",
      "          f:podName:\n",
      "          f:webUIAddress:\n",
      "          f:webUIPort:\n",
      "          f:webUIServiceName:\n",
      "        f:executionAttempts:\n",
      "        f:lastSubmissionAttemptTime:\n",
      "        f:sparkApplicationId:\n",
      "        f:submissionAttempts:\n",
      "        f:submissionID:\n",
      "        f:terminationTime:\n",
      "    Manager:         spark-operator\n",
      "    Operation:       Update\n",
      "    Time:            2021-03-17T12:32:27Z\n",
      "  Resource Version:  8475374\n",
      "  Self Link:         /apis/sparkoperator.k8s.io/v1beta2/namespaces/demo01/sparkapplications/horovod-mnist-0\n",
      "  UID:               2a54bcd0-29cd-4cb8-a6da-3787743c9da3\n",
      "Spec:\n",
      "  Arguments:\n",
      "    --epochs\n",
      "    10\n",
      "  Driver:\n",
      "    Annotations:\n",
      "      sidecar.istio.io/inject:  false\n",
      "    Cores:                      1\n",
      "    Env:\n",
      "      Name:   PYTHONUNBUFFERED\n",
      "      Value:  1\n",
      "    Labels:\n",
      "      Metrics - Exposed:  true\n",
      "      Version:            3.0.0\n",
      "    Memory:               1G\n",
      "    Service Account:      default-editor\n",
      "  Executor:\n",
      "    Annotations:\n",
      "      sidecar.istio.io/inject:  false\n",
      "    Cores:                      1\n",
      "    Instances:                  5\n",
      "    Labels:\n",
      "      Metrics - Exposed:  true\n",
      "      Version:            3.0.0\n",
      "    Memory:               512m\n",
      "  Image:                  mavencodev/sparkjob:1.0\n",
      "  Image Pull Policy:      Always\n",
      "  Main Application File:  local:///mnist.py\n",
      "  Mode:                   cluster\n",
      "  Monitoring:\n",
      "    Expose Driver Metrics:    true\n",
      "    Expose Executor Metrics:  true\n",
      "    Prometheus:\n",
      "      Jmx Exporter Jar:  /prometheus/jmx_prometheus_javaagent-0.11.0.jar\n",
      "      Port:              8090\n",
      "  Python Version:        3\n",
      "  Restart Policy:\n",
      "    Type:         Never\n",
      "  Spark Version:  3.0.0\n",
      "  Type:           Python\n",
      "Status:\n",
      "  Application State:\n",
      "    State:  SUBMITTED\n",
      "  Driver Info:\n",
      "    Pod Name:                    horovod-mnist-0-driver\n",
      "    Web UI Address:              172.20.132.239:4040\n",
      "    Web UI Port:                 4040\n",
      "    Web UI Service Name:         horovod-mnist-0-ui-svc\n",
      "  Execution Attempts:            1\n",
      "  Last Submission Attempt Time:  2021-03-17T12:32:27Z\n",
      "  Spark Application Id:          spark-acd92f42a41f4443812c37d2e4e994e2\n",
      "  Submission Attempts:           1\n",
      "  Submission ID:                 984d7346-bcf6-4782-b436-86bd6b05d7da\n",
      "  Termination Time:              <nil>\n",
      "Events:\n",
      "  Type    Reason                     Age   From            Message\n",
      "  ----    ------                     ----  ----            -------\n",
      "  Normal  SparkApplicationAdded      5s    spark-operator  SparkApplication horovod-mnist-0 was added, enqueuing it for submission\n",
      "  Normal  SparkApplicationSubmitted  2s    spark-operator  SparkApplication horovod-mnist-0 was submitted successfully\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe $HVD_JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkapplication.sparkoperator.k8s.io \"horovod-mnist-0\" deleted\n"
     ]
    }
   ],
   "source": [
    "! kubectl delete $HVD_JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the check to see if the pod is still up and running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"horovod-mnist\" not found\n"
     ]
    }
   ],
   "source": [
    "! kubectl -n demo01 logs -f horovod-mnist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
