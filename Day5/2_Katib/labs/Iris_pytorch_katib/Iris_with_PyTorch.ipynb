{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Iris with PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sze3QYr7GJeU"
      },
      "source": [
        "# Using the Pytorch Operator For Katib Hyperparameter Tuning\n",
        "\n",
        "Using PyTorch to build a model with two convolutional layers and two fully connected layers to perform the multi-class classification of images provided.\n",
        "\n",
        "\n",
        "### Requirement\n",
        "All you need is this notebook running in Kubeflow Notebook Server once you have cloned this repo\n",
        "You can use the pre-packaged Docker image here but if  you choose to build your own docker image, you must also have a Docker client installed on your machine.\n",
        "\n",
        "## Prerequisites\n",
        "Before we proceed, check to see if Pytorch is already installed in your Notebook Server Environment or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHQYCO6IGJee"
      },
      "source": [
        "# confirm if torch is installed\n",
        "! pip list | grep torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_o8VYE7GJeg"
      },
      "source": [
        "# if torch is not installed, run this\n",
        "! pip3 install --user ipywidgets \n",
        "! pip3 install --user torch torchvision matplotlib --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsipj_Q-GJeg"
      },
      "source": [
        "TRAINER_FILE = 'iris.py'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jWE5YETGJeg"
      },
      "source": [
        "## How to Train the Pytorch Model in the Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdTd6nEGGJeh"
      },
      "source": [
        "Since we ultimately want to train the model in a distributed mode, we put all the code in a single cell.\n",
        "That way we can save the file and include it in a container image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM-Ifjn2GJeh",
        "outputId": "fb2cae25-74bd-4fab-e71f-225fb850d098"
      },
      "source": [
        "%%writefile $TRAINER_FILE\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "WORLD_SIZE = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "names = iris['target_names']\n",
        "feature_names = iris['feature_names']\n",
        "\n",
        "# Scale data to have mean 0 and variance 1 \n",
        "# which is importance for convergence of the neural network\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=2)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(4, 50)\n",
        "        self.layer2 = nn.Linear(50, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            msg = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item())\n",
        "            logging.info(msg)\n",
        "            niter = epoch * len(train_loader) + batch_idx\n",
        "\n",
        "\n",
        "def test(args, model, device, test_loader, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    logging.info(\"{{metricName: accuracy, metricValue: {:.4f}}};{{metricName: loss, metricValue: {:.4f}}}\\n\".format(\n",
        "        float(correct) / len(test_loader.dataset), test_loss))\n",
        "\n",
        "\n",
        "def should_distribute():\n",
        "    return dist.is_available() and WORLD_SIZE > 1\n",
        "\n",
        "\n",
        "def is_distributed():\n",
        "    return dist.is_available() and dist.is_initialized()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=64, metavar=\"N\",\n",
        "                        help=\"input batch size for training (default: 64)\")\n",
        "    parser.add_argument(\"--test-batch-size\", type=int, default=1000, metavar=\"N\",\n",
        "                        help=\"input batch size for testing (default: 1000)\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30, metavar=\"N\",\n",
        "                        help=\"number of epochs to train (default: 10)\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.01, metavar=\"LR\",\n",
        "                        help=\"learning rate (default: 0.01)\")\n",
        "    parser.add_argument(\"--momentum\", type=float, default=0.5, metavar=\"M\",\n",
        "                        help=\"SGD momentum (default: 0.5)\")\n",
        "    parser.add_argument(\"--no-cuda\", action=\"store_true\", default=False,\n",
        "                        help=\"disables CUDA training\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
        "                        help=\"random seed (default: 1)\")\n",
        "    parser.add_argument(\"--log-interval\", type=int, default=10, metavar=\"N\",\n",
        "                        help=\"how many batches to wait before logging training status\")\n",
        "    parser.add_argument(\"--log-path\", type=str, default=\"\",\n",
        "                        help=\"Path to save logs. Print to StdOut if log-path is not set\")\n",
        "    parser.add_argument(\"--save-model\", action=\"store_true\", default=False,\n",
        "                        help=\"For Saving the current Model\")\n",
        "\n",
        "    if dist.is_available():\n",
        "        parser.add_argument(\"--backend\", type=str, help=\"Distributed backend\",\n",
        "                            choices=[dist.Backend.GLOO, dist.Backend.NCCL, dist.Backend.MPI],\n",
        "                            default=dist.Backend.GLOO)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Use this format (%Y-%m-%dT%H:%M:%SZ) to record timestamp of the metrics.\n",
        "    # If log_path is empty print log to StdOut, otherwise print log to the file.\n",
        "    if args.log_path == \"\":\n",
        "        logging.basicConfig(\n",
        "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
        "            level=logging.DEBUG)\n",
        "    else:\n",
        "        logging.basicConfig(\n",
        "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
        "            level=logging.DEBUG,\n",
        "            filename=args.log_path)\n",
        "\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    if use_cuda:\n",
        "        print(\"Using CUDA\")\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if should_distribute():\n",
        "        print(\"Using distributed PyTorch with {} backend\".format(args.backend))\n",
        "        dist.init_process_group(backend=args.backend)\n",
        "\n",
        "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "    # train dataset\n",
        "    train_dataset = torch.utils.data.TensorDataset(Tensor(X_train),Tensor(y_train).type(torch.LongTensor)) \n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                              batch_size=args.batch_size, \n",
        "                                              shuffle=True, **kwargs)\n",
        "\n",
        "    # test dataset\n",
        "    test_dataset = torch.utils.data.TensorDataset(Tensor(X_test),Tensor(y_test).type(torch.LongTensor)) \n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                              batch_size=args.test_batch_size, \n",
        "                                              shuffle=False, **kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "\n",
        "    if is_distributed():\n",
        "        Distributor = nn.parallel.DistributedDataParallel if use_cuda \\\n",
        "            else nn.parallel.DistributedDataParallelCPU\n",
        "        model = Distributor(model)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(args, model, device, test_loader, epoch)\n",
        "\n",
        "    if (args.save_model):\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting iris.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKvFgv8ZGJem",
        "outputId": "9677c11b-62e1-4709-cfbe-ddedaa7bed47"
      },
      "source": [
        "%run $TRAINER_FILE --epochs 20 --log-interval 128"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-25T02:30:53Z INFO     Train Epoch: 1 [0/120 (0%)]\tloss=1.0757\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.2667};{metricName: loss, metricValue: 1.0962}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 2 [0/120 (0%)]\tloss=1.0640\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.2667};{metricName: loss, metricValue: 1.0812}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 3 [0/120 (0%)]\tloss=1.0526\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.3333};{metricName: loss, metricValue: 1.0658}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 4 [0/120 (0%)]\tloss=1.0664\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.5667};{metricName: loss, metricValue: 1.0501}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 5 [0/120 (0%)]\tloss=1.0209\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 1.0348}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 6 [0/120 (0%)]\tloss=1.0092\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 1.0195}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 7 [0/120 (0%)]\tloss=0.9940\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 1.0047}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 8 [0/120 (0%)]\tloss=0.9723\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.9895}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 9 [0/120 (0%)]\tloss=0.9673\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.9754}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 10 [0/120 (0%)]\tloss=0.9482\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.9610}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 11 [0/120 (0%)]\tloss=0.9427\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.9465}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 12 [0/120 (0%)]\tloss=0.9286\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.9319}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 13 [0/120 (0%)]\tloss=0.9105\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.9174}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 14 [0/120 (0%)]\tloss=0.9305\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.9025}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 15 [0/120 (0%)]\tloss=0.9213\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.8877}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 16 [0/120 (0%)]\tloss=0.8750\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.8730}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 17 [0/120 (0%)]\tloss=0.8857\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.8583}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 18 [0/120 (0%)]\tloss=0.8254\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.8434}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 19 [0/120 (0%)]\tloss=0.8459\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.8286}\n",
            "\n",
            "2021-03-25T02:30:54Z INFO     Train Epoch: 20 [0/120 (0%)]\tloss=0.8179\n",
            "2021-03-25T02:30:54Z INFO     {metricName: accuracy, metricValue: 0.7000};{metricName: loss, metricValue: 0.8138}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5xesNhjGJer"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}