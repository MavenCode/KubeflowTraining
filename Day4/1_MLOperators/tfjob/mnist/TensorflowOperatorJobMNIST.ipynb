{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST Dataset using Tensorflow Operator\n",
    "\n",
    "## Prerequisites\n",
    "Before we proceed, let's check that we're using the right image, that is, [TensorFlow](https://www.tensorflow.org/api_docs/) is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 list | grep tensorflow \n",
    "! pip3 install --user tensorflow==2.1.0\n",
    "! pip3 install --user ipywidgets tensorflow-datasets nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"tfjob.py\"\n",
    "KUBERNETES_FILE = \"tfjob-mnist.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to capture output from a cell with [`%%capture`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture) that usually looks like `some-resource created`.\n",
    "To that end, let's define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}. It may already exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Load and Inspect the Data\n",
    "We grab the MNIST data set with the aid of `tensorflow_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "mnist, info = tfds.load('mnist', split='train', shuffle_files=True , with_info=True)\n",
    "tfds.show_examples(mnist, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily read off the shape of the input tensors that shows the images are all 28x28 pixels, but we do not yet know whether their greyscale values have been scaled to the [0, 1] range or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in mnist.take(1):\n",
    "    squeezed = tf.squeeze(example[\"image\"])\n",
    "    print(tf.math.reduce_min(squeezed), tf.math.reduce_max(squeezed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, they have not.\n",
    "This means we have to do this in the training and before serving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear variables as we have no need for these any longer\n",
    "del mnist, squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Train the Model in the Notebook\n",
    "We want to train the model in a distributed fashion, we put all the code in a single cell.\n",
    "That way we can save the file and include it in a container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "trainer_code"
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "  BUFFER_SIZE = 10000\n",
    "\n",
    "  datasets, ds_info = tfds.load(name=\"mnist\", download=True, with_info=True, as_supervised=True)\n",
    "  mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
    "\n",
    "  def scale(image, label):\n",
    "      image = tf.cast(image, tf.float32) / 255\n",
    "      return image, label\n",
    "\n",
    "  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "  test_dataset = mnist_test.map(scale)\n",
    "\n",
    "  return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def model(args):\n",
    "  model = models.Sequential()\n",
    "  model.add(\n",
    "      layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "  model.add(layers.MaxPooling2D((2, 2)))\n",
    "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(256, activation='relu'))\n",
    "  model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  model.summary()\n",
    "  opt = args.optimizer\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  tf.keras.backend.set_value(model.optimizer.learning_rate, args.learning_rate)\n",
    "  return model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "  # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "  # layers on each device across all workers\n",
    "  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "      communication=tf.distribute.experimental.CollectiveCommunication.AUTO)\n",
    "  logging.debug(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
    "  BATCH_SIZE_PER_REPLICA = args.batch_size\n",
    "  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "  # Datasets need to be created after instantiation of `MultiWorkerMirroredStrategy`\n",
    "  train_dataset, test_dataset = make_datasets_unbatched()\n",
    "  train_dataset = train_dataset.batch(batch_size=BATCH_SIZE)\n",
    "  test_dataset = test_dataset.batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "  # See: https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions\n",
    "  options = tf.data.Options()\n",
    "  options.experimental_distribute.auto_shard_policy = \\\n",
    "        tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "  train_datasets_sharded  = train_dataset.with_options(options)\n",
    "  test_dataset_sharded = test_dataset.with_options(options)\n",
    "\n",
    "  with strategy.scope():\n",
    "    # Model building/compiling need to be within `strategy.scope()`.\n",
    "    multi_worker_model = model(args)\n",
    "\n",
    "  # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "  # number of steps per epoch. \n",
    "  multi_worker_model.fit(train_datasets_sharded,\n",
    "                         epochs=10,\n",
    "                         steps_per_epoch=10)\n",
    "  \n",
    "  eval_loss, eval_acc = multi_worker_model.evaluate(test_dataset_sharded, \n",
    "                                                    verbose=0, steps=10)\n",
    "\n",
    "  # Log metrics for Katib\n",
    "  logging.info(\"loss={:.4f}\".format(eval_loss))\n",
    "  logging.info(\"accuracy={:.4f}\".format(eval_acc))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--batch_size\",\n",
    "                      type=int,\n",
    "                      default=128,\n",
    "                      metavar=\"N\",\n",
    "                      help=\"Batch size for training (default: 128)\")\n",
    "  parser.add_argument(\"--learning_rate\", \n",
    "                      type=float,  \n",
    "                      default=0.001,\n",
    "                      metavar=\"N\",\n",
    "                      help='Initial learning rate')\n",
    "  parser.add_argument(\"--optimizer\", \n",
    "                      type=str, \n",
    "                      default='adam',\n",
    "                      metavar=\"N\",\n",
    "                      help='optimizer')\n",
    "\n",
    "  parsed_args, _ = parser.parse_known_args()\n",
    "  main(parsed_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That saves the file as defined by `TRAINER_FILE` but it does not run it.\n",
    "\n",
    "Let's see if our code is correct by running it from within our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run $TRAINER_FILE --optimizer $optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "trainer_dockerfile"
    ]
   },
   "source": [
    "## How to Create a Docker Image Manually\n",
    "\n",
    "\n",
    "The Dockerfile looks as follows:\n",
    "\n",
    "```\n",
    "FROM tensorflow/tensorflow:2.4.0\n",
    "RUN pip install tensorflow_datasets\n",
    "COPY tfjob.py /\n",
    "ENTRYPOINT [\"python\", \"/tfjob.py\", \"--batch_size\", \"100\", \"--learning_rate\", \"0.001\", \"--optimizer\", \"adam\"]\n",
    "```\n",
    "\n",
    "If GPU support is not needed, you can leave off the `-gpu` suffix from the image.\n",
    "`mnist.py` is the trainer code you have to download to your local machine.\n",
    "\n",
    "Then it's easy to push images to your container registry:\n",
    "\n",
    "```bash\n",
    "docker build -t <docker_image_name_with_tag> .\n",
    "docker push <docker_image_name_with_tag>\n",
    "```\n",
    "\n",
    "The image is available as `mavencodev/tf_job:5.0` in case you want to skip it for now.\n",
    "\n",
    "## How to Create a Distributed `TFJob`\n",
    "For large training jobs, we wish to run our trainer in a distributed mode.\n",
    "Once the notebook server cluster can access the Docker image from the registry, we can launch a distributed PyTorch job.\n",
    "\n",
    "The specification for a distributed `TFJob` is defined using YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: \"TFJob\"\n",
    "metadata:\n",
    "  name: \"mnistjob\"\n",
    "  namespace: demo01 # your-user-namespace\n",
    "spec:\n",
    "  cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: 2\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: tensorflow\n",
    "            # modify this property if you would like to use a custom image\n",
    "            image: mavencodev/tf_job:5.0\n",
    "            command:\n",
    "                - \"python\"\n",
    "                - \"/tfjob.py\"\n",
    "                - \"--batch_size=150\"\n",
    "                - \"--learning_rate=0.001\"\n",
    "                - \"--optimizer=adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy the distributed training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture tf_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_JOB = get_resource(tf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the job status, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl describe $TF_JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the created pods matching the specified number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods -l job-name=mnistjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of issues, it may be helpful to see the last ten events within the cluster:\n",
    "\n",
    "```bash\n",
    "! kubectl get events --sort-by='.lastTimestamp' | tail\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get events --sort-by='.lastTimestamp' | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stream logs from the worker-0 pod to check the training progress, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl logs -f mnist-worker-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the job, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete tfjob --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if the check to see if the pod is still up and running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n demo01 logs -f mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
