{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipeline with Mnist Dataset\n",
    "\n",
    "## Introduction\n",
    "[Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/) helps with building entire workflows\n",
    "\n",
    "These steps can be triggered automatically by a CI/CD workflow or on demand from a command line or notebook.\n",
    "\n",
    "\n",
    "**Components** performs a single step in a Machine Learning workflow such (e.g. data ingestion, data preprocessing, data transformation, model training, hyperparameter tuning). \n",
    "\n",
    "**Dataset**: The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available in Tenserflow Datasets. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "## Prerequisites\n",
    "check to see if kfp is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure access Minio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our Training Dataset to Minio\n",
    "\n",
    "First, we configure credentials for `mc`, the MinIO command line client.\n",
    "We then use it to create a bucket, upload the dataset to it, and set access policy so that the pipeline can download it from MinIO.\n",
    "\n",
    "Follow the steps below to download minio client\n",
    "<div class=\"alert\">\n",
    "   <code>\n",
    "    wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "    chmod +x mc\n",
    "    ./mc --help\n",
    "    </code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "! chmod +x mc\n",
    "! ./mc --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Connect to the Minio Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Create a bucket to store your data and export your model to Minio\n",
    "\n",
    "**Make sure you clear this bucket once you are cone running your pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc mb minio/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Upload the dataset to your bucket in Minio.\n",
    "\n",
    "**Note**: Make sure you have your dataset in a folder like we have here as <code>datasets</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar --dereference -czf datasets.tar.gz ./datasets\n",
    "! ./mc cp datasets.tar.gz minio/mnist/datasets.tar.gz\n",
    "! ./mc policy set download minio/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded your data too many times while testing, use the following code to clear out your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rm --recursive --force minio/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minio Server URL and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "MINIO_ACCESS_KEY='minio'\n",
    "MINIO_SECRET_KEY='minio123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Implement Kubeflow Pipelines Components\n",
    "\n",
    "In this pipeline, we have the following components:\n",
    "- MNIST dataset download component\n",
    "- Train the TensorFlow model\n",
    "- Evaluate the trained model\n",
    "- Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath #helps define the input & output between the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: Download the MNIST Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(minio_server: str, data_dir: OutputPath(str)):\n",
    "    \"\"\"Download the MNIST data set to the KFP volume to share it among all steps\"\"\"\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        \n",
    "    #this url leads to your bucket\n",
    "    url = f'http://{minio_server}/mnist/datasets.tar.gz'\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
    "    tar.extractall(path=data_dir)\n",
    "    \n",
    "    subprocess.call([\"ls\", \"-lha\", data_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Train the Model\n",
    "For both the training and evaluation we must divide the integer-valued pixel values by 255 to scale all values into the [0, 1] (floating-point) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
    "    \"\"\"Trains a single-layer CNN for 5 epochs using a pre-downloaded dataset.\n",
    "    Once trained, the model is persisted to `model_dir`.\"\"\"\n",
    "\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    def normalize_image(image, label):\n",
    "        \"\"\"Normalizes images: `uint8` -> `float32`\"\"\"\n",
    "        return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    print(model.summary())\n",
    "    ds_train, ds_info = tfds.load(\n",
    "        \"mnist\",\n",
    "        split=\"train\",\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "        download=True,\n",
    "        data_dir=f\"{data_dir}/datasets\",\n",
    "    )\n",
    "\n",
    "    ds_train = ds_train.map(\n",
    "        normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    model.fit(\n",
    "        ds_train,\n",
    "        epochs=5,\n",
    "    )\n",
    "\n",
    "    model.save(model_dir)\n",
    "    print(f\"Model saved {model_dir}\")\n",
    "    print(os.listdir(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Evaluate the Model\n",
    "With the following Python function the model is evaluated.\n",
    "The metrics [metadata](https://www.kubeflow.org/docs/pipelines/sdk/pipelines-metrics/) (loss and accuracy) is available to the Kubeflow Pipelines UI.\n",
    "Metadata can automatically be visualized with output viewer(s).\n",
    "Please go [here](https://www.kubeflow.org/docs/pipelines/sdk/output-viewer/) to see how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n",
    "    metadata.\"\"\"\n",
    "\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    from collections import namedtuple\n",
    "\n",
    "    def normalize_image(image, label):\n",
    "        return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "    ds_test, ds_info = tfds.load(\n",
    "        \"mnist\",\n",
    "        split=\"test\",\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "        download=True,\n",
    "        data_dir=f\"{data_dir}/datasets\",\n",
    "    )\n",
    "\n",
    "    ds_test = ds_test.map(\n",
    "        normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    (loss, accuracy) = model.evaluate(ds_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    model_dir: InputPath(str),\n",
    "    metrics: InputPath(str),\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "    \n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=f'http://{minio_server}',\n",
    "        aws_access_key_id=minio_access_key,\n",
    "        aws_secret_access_key=minio_secret_key,\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    print(response , export_bucket)\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, model_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}/{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Combine the Components into a Pipeline\n",
    "Note that up to this point we have not yet used the Kubeflow Pipelines SDK!\n",
    "\n",
    "With our four components (i.e. self-contained functions) defined, we can wire up the dependencies with Kubeflow Pipelines.\n",
    "\n",
    "The call [`components.func_to_container_op(f, base_image=img)(*args)`](https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/) has the following ingredients:\n",
    "- `f` is the Python function that defines a component\n",
    "- `img` is the base (Docker) image used to package the function\n",
    "- `*args` lists the arguments to `f`\n",
    "\n",
    "What the `*args` mean is best explained by going forward through the graph:\n",
    "- `downloadOp` is the very first step and has no dependencies; it therefore has no `InputPath`.\n",
    "  Its output (i.e. `OutputPath`) is stored in `data_dir`.\n",
    "- `trainOp` needs the data downloaded from `downloadOp` and its signature lists `data_dir` (input) and `model_dir` (output).\n",
    "  So, it _depends on_ `downloadOp.output` (i.e. the previous step's output) and stores its own outputs in `model_dir`, which can be used by another step.\n",
    "  `downloadOp` is the parent of `trainOp`, as required.\n",
    "- `evaluateOp`'s function takes three arguments: `data_dir` (i.e. `downloadOp.output`), `model_dir` (i.e. `trainOp.output`), and `metrics_path`, which is where the function stores its evaluation metrics.\n",
    "  That way, `evaluateOp` can only run after the successful completion of both `downloadOp` and `trainOp`.\n",
    "- `exportOp` runs the function `export_model`, which accepts five parameters: `model_dir`, `metrics`, `export_bucket`, `model_name`, and `model_version`.\n",
    "  From where do we get the `model_dir`?\n",
    "  It is nothing but `trainOp.output`.\n",
    "  Similarly, `metrics` is `evaluateOp.output`.\n",
    "  The remaining three arguments are regular Python arguments that are static for the pipeline: they do not depend on any step's output being available.\n",
    "  Hence, they are defined without using `InputPath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pipeline(\n",
    "    data_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mavencodev/minio:v.0.1\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )(minio_server)\n",
    "\n",
    "    trainOp = components.func_to_container_op(train_model, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "\n",
    "    evaluateOp = components.func_to_container_op(evaluate_model, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output, trainOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        trainOp.output, evaluateOp.output, export_bucket, \n",
    "        model_name, model_version, minio_server, minio_access_key, minio_secret_key\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case it isn't obvious: this will build the Docker images for you.\n",
    "Each image is based on `BASE_IMAGE` and includes the Python functions as executable files.\n",
    "Each component _can_ use a different base image though.\n",
    "This may come in handy if you want to have reusable components for automatic data and/or model analysis (e.g. to investigate bias).\n",
    "\n",
    "We still have to define the pipeline itself.\n",
    "Our `train_and_export` function defines dependencies but we must use the KFP domain-specific language (DSL) to register the pipeline with its components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-End MNIST Pipeline\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation and export\",\n",
    ")\n",
    "def mnist_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    export_bucket: str = \"mnist\",\n",
    "    model_name: str = \"mnist\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "    MINIO_ACCESS_KEY='minio'\n",
    "    MINIO_SECRET_KEY='minio123'\n",
    "    \n",
    "    \n",
    "    train_model_pipeline(\n",
    "        data_dir=data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        minio_server=MINIO_SERVER,\n",
    "        minio_access_key=MINIO_ACCESS_KEY,\n",
    "        minio_secret_key=MINIO_SECRET_KEY,\n",
    "    )\n",
    "    \n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, let's submit the pipeline directly from our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = mnist_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End-Demo\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  'mnist.yaml')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the generated yaml file to create a pipeline in Kubeflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now delete your bucket when you have run the pipeline successfully in the Kubeflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rb minio/mnist --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
