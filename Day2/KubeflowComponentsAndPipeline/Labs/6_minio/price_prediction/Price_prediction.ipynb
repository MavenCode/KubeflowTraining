{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for House Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/) helps with building entire workflows\n",
    "\n",
    "These steps can be triggered automatically by a CI/CD workflow or on demand from a command line or notebook.\n",
    "\n",
    "\n",
    "**Components** performs a single step in a Machine Learning workflow such (e.g. data ingestion, data preprocessing, data transformation, model training, hyperparameter tuning). \n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "Acquiring properties is common in our society today. However, a guildline for interested/prospected buyers to help them get a good value for their money seems to be lacking and buyers are left at their fate to gamble different options with their hard earned money. This project seeks to provide a model to guide buyers predict the price of a house based on their choices' features of a house.\n",
    "\n",
    "### Data:\n",
    "The data was collected within the 3 months, the feautures include city, number of bedrooms, number of bathrooms, square of living area, square of basement, number of floors, waterfront, number of views, year built, year renovated, etc\n",
    "\n",
    "## Prerequisites\n",
    "check to see if kfp is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Name: kfp\n",
      "Version: 1.4.0\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: UNKNOWN\n",
      "Author: google\n",
      "Author-email: None\n",
      "License: UNKNOWN\n",
      "Location: /home/jovyan/.local/lib/python3.6/site-packages\n",
      "Requires: tabulate, docstring-parser, Deprecated, google-cloud-storage, cloudpickle, click, fire, kfp-pipeline-spec, PyYAML, kubernetes, google-auth, jsonschema, requests-toolbelt, strip-hints, kfp-server-api\n",
      "Required-by: kfp-notebook\n"
     ]
    }
   ],
   "source": [
    "# You may need to restart your notebook kernel after updating the kfp sdk\n",
    "! pip3 show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure access Minio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our Training Dataset to Minio\n",
    "\n",
    "First, we configure credentials for `mc`, the MinIO command line client.\n",
    "We then use it to create a bucket, upload the dataset to it, and set access policy so that the pipeline can download it from MinIO.\n",
    "\n",
    "Follow the steps below to download minio client\n",
    "<div class=\"alert\">\n",
    "   <code>\n",
    "    wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "    chmod +x mc\n",
    "    ./mc --help\n",
    "    </code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-21 06:42:50--  https://dl.min.io/client/mc/release/linux-amd64/mc\n",
      "Resolving dl.min.io (dl.min.io)... 178.128.69.202\n",
      "Connecting to dl.min.io (dl.min.io)|178.128.69.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20287488 (19M) [application/octet-stream]\n",
      "Saving to: ‘mc.5’\n",
      "\n",
      "mc.5                 19%[==>                 ]   3.68M   488KB/s    eta 33s    "
     ]
    }
   ],
   "source": [
    "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "! chmod +x mc\n",
    "! ./mc --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Connect to the Minio Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Create a bucket to stor your data and export your model to Minio\n",
    "\n",
    "**Make sure you clear this bucket once you are cone running your pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc mb minio/house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Upload the dataset to your bucket in Minio.\n",
    "\n",
    "**Note**: Make sure you have your dataset in a folder like we have here as <code>datasets</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar --dereference -czf datasets.tar.gz ./datasets\n",
    "! ./mc cp datasets.tar.gz minio/house/datasets.tar.gz\n",
    "! ./mc policy set download minio/house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded your data too many times while testing, use the following code to clear out your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rm --recursive --force minio/price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minio Server URL and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "MINIO_ACCESS_KEY='minio'\n",
    "MINIO_SECRET_KEY='minio123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Implement Kubeflow Pipelines Components\n",
    "\n",
    "In this pipeline, we have the following components:\n",
    "- House price dataset download component\n",
    "- Preprocess the dataset component\n",
    "- Train the model component\n",
    "- Make pridictions component\n",
    "- Export the trained model component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath #helps define the input & output between the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: Download the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(minio_server: str, data_dir: OutputPath(str)):\n",
    "    \"\"\"Download the data set to the KFP volume to share it among all steps\"\"\"\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = f'http://{minio_server}/house/datasets.tar.gz'\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
    "    tar.extractall(path=data_dir)\n",
    "    \n",
    "    subprocess.call([\"pwd\", data_dir])\n",
    "    subprocess.call([\"ls\", \"-lha\", data_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_dir: InputPath(str), clean_data_dir: OutputPath(str)):\n",
    "    \n",
    "    import numpy as np\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import train_test_split  # splitting the data\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    # Get data\n",
    "    \n",
    "    subprocess.call([\"ls\", \"-dlha\", f'{data_dir}/datasets/data.csv'])\n",
    "    data = pd.read_csv(f\"{data_dir}/datasets/data.csv\")\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    # drop unneccessary column\n",
    "#     data.drop(columns=['date','country', 'statezip', 'street'], inplace=True)\n",
    "    \n",
    "    #Filtering for prices that are not zero.\n",
    "    data = data.query('price != 0')\n",
    "    \n",
    "    #Filtering for houses not zero for number of bedrooms and bathrooms\n",
    "    data = data.query('bedrooms != 0' or 'bathrooms != 0')\n",
    "    \n",
    "    # Converting the city variable to numerical values.\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    data.loc[data[\"city\"]] = le.fit_transform(data[\"city\"])\n",
    "    \n",
    "    #year convert function\n",
    "    def yr_col (col1, col2):\n",
    "        if col1 == 0:\n",
    "            col1 = col2\n",
    "        else:\n",
    "            col1\n",
    "        return col1\n",
    "    \n",
    "    data.drop(columns=['yr_renovated'], inplace=True)\n",
    "    #Change year renovated column with zero entry to the year built.\n",
    "#     data['yr_renovated'] = data.apply(lambda x: yr_col(x['yr_renovated'], x['yr_built']), axis =1)\n",
    "    \n",
    "    #Filtering the outliers\n",
    "    data = data.loc[\n",
    "                (data[\"price\"] <= 2000000) & \n",
    "                (data[\"price\"] > 150000) & \n",
    "                (data[\"bathrooms\"] <= 4.5) & \n",
    "                (data[\"condition\"] > 2) & \n",
    "                (data[\"sqft_living\"] > 700) & \n",
    "                (data[\"sqft_living\"] <= 5000) & \n",
    "                (data[\"sqft_lot\"] <= 50000) & \n",
    "                (data[\"sqft_above\"] <= 5000) &\n",
    "                (data[\"sqft_basement\"] <= 5000) &\n",
    "                (data[\"bedrooms\"] <= 6) \n",
    "                ]\n",
    "    \n",
    "    #Filtering for multicollinearity\n",
    "    data = data.drop(columns=['sqft_living', 'sqft_above'])\n",
    "    \n",
    "    # We normalise our dataset to a common scale using the min max scaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "    \n",
    "    # split the data into X and y\n",
    "    X = data.drop(['price'], axis=1)  # predictor\n",
    "    y = data.loc[:,'price'] # target\n",
    "    \n",
    "    # Split the data into training and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    \n",
    "    data = {\"X_train\": X_train,\"X_test\": X_test, \"Y_train\": y_train,\"Y_test\": y_test}\n",
    "    \n",
    "    os.makedirs(clean_data_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"clean_data.pickle {clean_data_dir}\")\n",
    "    \n",
    "    print(os.listdir(clean_data_dir))\n",
    "    \n",
    "    print(\"Preprocessing Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 3: Training the data with the Catboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clean_data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
    "    \n",
    "    # Install all the dependencies inside the function\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost==0.24.2'])\n",
    "    import pandas as pd\n",
    "    # import libraries for training\n",
    "    from catboost import CatBoostRegressor\n",
    "    \n",
    "    #load the preprocessed data\n",
    "    \n",
    "    print(clean_data_dir)\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    \n",
    "    # Instantiating the model \n",
    "    model = CatBoostRegressor(verbose=1, n_estimators=10)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    #Save the model to the designated \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(model_dir,'model.pickle'), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    print(f\"model.pickle {model_dir}\")\n",
    "    \n",
    "    print(os.listdir(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 4: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(\n",
    "    clean_data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):   \n",
    "    import pickle\n",
    "    import os\n",
    "    import sys, subprocess;\n",
    "    import numpy as np\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost==0.24.2'])\n",
    "    # Evaluation metrics\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from collections import namedtuple\n",
    "      \n",
    "    print(model_dir)\n",
    "    with open(os.path.join(model_dir,'model.pickle'), 'rb') as f:\n",
    "        model = pickle.load(f)        \n",
    "    print(model)\n",
    "    \n",
    "    print(clean_data_dir)\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)       \n",
    "    print(data)\n",
    " \n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    \n",
    "    #Evaluate the model and print the results\n",
    "    model_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    r2_score = r2_score(y_test, model_pred)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, model_pred))\n",
    "    rmse_train=np.sqrt(mean_squared_error(y_train, model.predict(X_train)))\n",
    "    \n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"r2_score\", \"numberValue\": str(r2_score), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"rmse_test\", \"numberValue\": str(rmse_test), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"rmse_train\", \"numberValue\": str(rmse_train), \"format\": \"PERCENTAGE\"}\n",
    "            \n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    model_dir: InputPath(str),\n",
    "    metrics: InputPath(str),\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=f'http://{minio_server}',\n",
    "        aws_access_key_id=minio_access_key,\n",
    "        aws_secret_access_key=minio_secret_key,\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, model_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}/{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Components into a Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pipeline(\n",
    "    data_dir: str,\n",
    "    clean_data_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mavencodev/minio:v.0.1\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )(minio_server)\n",
    "    \n",
    "    preprocessOp = components.func_to_container_op(preprocess, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "    trainOp = components.func_to_container_op(train_model, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output\n",
    "    )\n",
    "\n",
    "    predictionOp = components.func_to_container_op(prediction, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output, trainOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        trainOp.output, predictionOp.output, export_bucket, model_name, model_version, \n",
    "        minio_server, minio_access_key, minio_secret_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-End House Prediction\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation and export\",\n",
    ")\n",
    "def price_prediction_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    clean_data_dir: str= \"/train/data\",\n",
    "    export_bucket: str = \"house\",\n",
    "    model_name: str = \"house\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "    MINIO_ACCESS_KEY='minio'\n",
    "    MINIO_SECRET_KEY='minio123'\n",
    "    \n",
    "    train_model_pipeline(\n",
    "        data_dir=data_dir,\n",
    "        clean_data_dir=clean_data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        minio_server=MINIO_SERVER,\n",
    "        minio_access_key=MINIO_ACCESS_KEY,\n",
    "        minio_secret_key=MINIO_SECRET_KEY,\n",
    "    )\n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, let's submit the pipeline directly from our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = price_prediction_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End-Demo\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  'price_prediction.yaml')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the generated yaml file to create a pipeline in Kubeflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now delete your bucket when you have run the pipeline successfully in the Kubeflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rb minio/price --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
