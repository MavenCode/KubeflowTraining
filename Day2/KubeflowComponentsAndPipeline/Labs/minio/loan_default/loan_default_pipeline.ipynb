{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B_RtgcNyiJP8",
    "outputId": "2d782497-dff3-444d-8065-613056972bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Name: kfp\n",
      "Version: 1.4.0\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: UNKNOWN\n",
      "Author: google\n",
      "Author-email: None\n",
      "License: UNKNOWN\n",
      "Location: /home/jovyan/.local/lib/python3.6/site-packages\n",
      "Requires: Deprecated, PyYAML, jsonschema, requests-toolbelt, kfp-pipeline-spec, kubernetes, strip-hints, google-auth, docstring-parser, kfp-server-api, fire, google-cloud-storage, click, cloudpickle, tabulate\n",
      "Required-by: kfp-notebook\n"
     ]
    }
   ],
   "source": [
    "! pip3 show kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-19 22:08:57--  https://dl.min.io/client/mc/release/linux-amd64/mc\n",
      "Resolving dl.min.io (dl.min.io)... 178.128.69.202\n",
      "Connecting to dl.min.io (dl.min.io)|178.128.69.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20287488 (19M) [application/octet-stream]\n",
      "Saving to: ‘mc.5’\n",
      "\n",
      "mc.5                100%[===================>]  19.35M  2.89MB/s    in 20s     \n",
      "\n",
      "2021-03-19 22:09:18 (968 KB/s) - ‘mc.5’ saved [20287488/20287488]\n",
      "\n",
      "NAME:\n",
      "  mc - MinIO Client for cloud storage and filesystems.\n",
      "\n",
      "USAGE:\n",
      "  mc [FLAGS] COMMAND [COMMAND FLAGS | -h] [ARGUMENTS...]\n",
      "\n",
      "COMMANDS:\n",
      "  alias      set, remove and list aliases in configuration file\n",
      "  ls         list buckets and objects\n",
      "  mb         make a bucket\n",
      "  rb         remove a bucket\n",
      "  cp         copy objects\n",
      "  mirror     synchronize object(s) to a remote site\n",
      "  cat        display object contents\n",
      "  head       display first 'n' lines of an object\n",
      "  pipe       stream STDIN to an object\n",
      "  share      generate URL for temporary access to an object\n",
      "  find       search for objects\n",
      "  sql        run sql queries on objects\n",
      "  stat       show object metadata\n",
      "  mv         move objects\n",
      "  tree       list buckets and objects in a tree format\n",
      "  du         summarize disk usage recursively\n",
      "  retention  set retention for object(s)\n",
      "  legalhold  manage legal hold for object(s)\n",
      "  diff       list differences in object name, size, and date between two buckets\n",
      "  rm         remove objects\n",
      "  version    manage bucket versioning\n",
      "  ilm        manage bucket lifecycle\n",
      "  encrypt    manage bucket encryption config\n",
      "  event      manage object notifications\n",
      "  watch      listen for object notification events\n",
      "  undo       undo PUT/DELETE operations\n",
      "  policy     manage anonymous access to buckets and objects\n",
      "  tag        manage tags for bucket and object(s)\n",
      "  replicate  configure server side bucket replication\n",
      "  admin      manage MinIO servers\n",
      "  update     update mc to latest release\n",
      "  \n",
      "GLOBAL FLAGS:\n",
      "  --autocompletion              install auto-completion for your shell\n",
      "  --config-dir value, -C value  path to configuration folder (default: \"/home/jovyan/.mc\")\n",
      "  --quiet, -q                   disable progress bar display\n",
      "  --no-color                    disable color theme\n",
      "  --json                        enable JSON lines formatted output\n",
      "  --debug                       enable debug output\n",
      "  --insecure                    disable SSL certificate verification\n",
      "  --help, -h                    show help\n",
      "  --version, -v                 print the version\n",
      "  \n",
      "TIP:\n",
      "  Use 'mc --autocompletion' to enable shell autocompletion\n",
      "\n",
      "VERSION:\n",
      "  RELEASE.2021-03-12T03-36-59Z\n"
     ]
    }
   ],
   "source": [
    "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "! chmod +x mc\n",
    "! ./mc --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32mAdded `minio` successfully.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc mb minio/loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32m[2021-03-19 22:05:14 UTC]\u001b[0m\u001b[33m     0B\u001b[0m\u001b[36;1m loan/\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-03-19 21:59:29 UTC]\u001b[0m\u001b[33m     0B\u001b[0m\u001b[36;1m mlpipeline/\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-03-19 04:09:14 UTC]\u001b[0m\u001b[33m     0B\u001b[0m\u001b[36;1m mnist/\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc ls minio mlpipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...ts.tar.gz:  5.59 MiB / 5.59 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 25.04 MiB/s 0s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1mAccess permission for `minio/loan` is set to `download`\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! tar --dereference -czf datasets.tar.gz ./datasets\n",
    "! ./mc cp datasets.tar.gz minio/loan/datasets.tar.gz\n",
    "! ./mc policy set download minio/loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath #helps define the input & output between the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(data_dir: OutputPath(str)):\n",
    "    \"\"\"Download the data set to the KFP volume to share it among all steps\"\"\"\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = \"http://minio-service.kubeflow:9000/mlpipeline/datasets.tar.gz\"\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
    "    tar.extractall(path=data_dir)\n",
    "    \n",
    "    subprocess.call([\"ls\", \"-lha\", data_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_dir: InputPath(str), clean_data_dir: OutputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklearn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'imblearn'])\n",
    "#     subprocess.run([sys.executable, '-m', 'pip', 'install', 'os'])\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import re\n",
    "    import os\n",
    "    import pickle\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    df = pd.read_csv(f\"{data_dir}/datasets/data.csv\")\n",
    "\n",
    "    df =df.dropna(axis = 0 , how = 'any')\n",
    "\n",
    "    def age(dob):\n",
    "        yr = int(dob[-2:])  \n",
    "        if yr >=0 and yr < 20:\n",
    "            return yr + 2000\n",
    "        else:\n",
    "            return yr + 1900\n",
    "\n",
    "    df['Date.of.Birth'] = df['Date.of.Birth'].apply(age)\n",
    "    df['DisbursalDate'] = df['DisbursalDate'].apply(age)\n",
    "\n",
    "    df['Age'] = df['DisbursalDate'] - df['Date.of.Birth']\n",
    "    df = df.drop( ['DisbursalDate', 'Date.of.Birth'], axis=1)\n",
    "\n",
    "    df['AVERAGE.ACCT.AGE_yrs'] = df['AVERAGE.ACCT.AGE'].apply(lambda x: re.search(r'\\d+(?=yrs)', x).group(0)).astype(np.int)\n",
    "    df['AVERAGE.ACCT.AGE_mon'] = df['AVERAGE.ACCT.AGE'].apply(lambda x: re.search(r'\\d+(?=mon)', x).group(0)).astype(np.int)\n",
    "    df = df.drop('AVERAGE.ACCT.AGE', axis=1)\n",
    "\n",
    "    df['CREDIT.HISTORY.LENGTH_yrs'] = df['CREDIT.HISTORY.LENGTH'].apply(lambda x: re.search(r'\\d+(?=yrs)', x).group(0)).astype(np.int)\n",
    "    df['CREDIT.HISTORY.LENGTH_mon'] = df['CREDIT.HISTORY.LENGTH'].apply(lambda x: re.search(r'\\d+(?=mon)', x).group(0)).astype(np.int)\n",
    "    df = df.drop('CREDIT.HISTORY.LENGTH', axis=1)\n",
    "    pri_columns = ['PRI.NO.OF.ACCTS','SEC.NO.OF.ACCTS',\n",
    "            'PRI.ACTIVE.ACCTS','SEC.ACTIVE.ACCTS',\n",
    "            'PRI.OVERDUE.ACCTS','SEC.OVERDUE.ACCTS',\n",
    "            'PRI.CURRENT.BALANCE','SEC.CURRENT.BALANCE',\n",
    "            'PRI.SANCTIONED.AMOUNT','SEC.SANCTIONED.AMOUNT',\n",
    "            'PRI.DISBURSED.AMOUNT','SEC.DISBURSED.AMOUNT',\n",
    "            'PRIMARY.INSTAL.AMT', 'SEC.INSTAL.AMT']\n",
    "\n",
    "    #Creating and Sorting Columns\n",
    "\n",
    "    df['NO_OF_ACCTS'] = df['PRI.NO.OF.ACCTS'] + df['SEC.NO.OF.ACCTS']\n",
    "\n",
    "    df['ACTIVE_ACCTS'] = df['PRI.ACTIVE.ACCTS'] + df['SEC.ACTIVE.ACCTS']\n",
    "\n",
    "    df['OVERDUE_ACCTS'] = df['PRI.OVERDUE.ACCTS'] + df['SEC.OVERDUE.ACCTS']\n",
    "\n",
    "    df['CURRENT_BALANCE'] = df['PRI.CURRENT.BALANCE'] + df['SEC.CURRENT.BALANCE']\n",
    "\n",
    "    df['SANCTIONED_AMOUNT'] = df['PRI.SANCTIONED.AMOUNT'] + df['SEC.SANCTIONED.AMOUNT']\n",
    "\n",
    "    df['Total_AMOUNT'] = df['PRI.DISBURSED.AMOUNT'] + df['SEC.DISBURSED.AMOUNT']\n",
    "\n",
    "    df['INSTAL_AMT'] = df['PRIMARY.INSTAL.AMT'] + df['SEC.SANCTIONED.AMOUNT']\n",
    "\n",
    "    df['INACTIVE_ACCTS'] = df['NO_OF_ACCTS'] - df['ACTIVE_ACCTS']\n",
    "\n",
    "    df.drop(pri_columns, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    columns_unique = ['UniqueID','MobileNo_Avl_Flag',\n",
    "          'Current_pincode_ID','Employee_code_ID',\n",
    "          'NO.OF_INQUIRIES','State_ID',\n",
    "          'branch_id','manufacturer_id','supplier_id', 'Driving_flag',\t'Passport_flag']\n",
    "    df = df.drop(columns=columns_unique)\n",
    "\n",
    "    objects = df.select_dtypes('object').columns.tolist()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[objects] = df[objects].apply(le.fit_transform) \n",
    "\n",
    "    X = df.drop(['loan_default'], axis=1)\n",
    "    y = df['loan_default']\n",
    "\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "    # Split the data into training and testing sets \n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = .3, random_state = 33)\n",
    "\n",
    "\n",
    "    sm = SMOTE(random_state=2)\n",
    "    x_train, y_train = sm.fit_resample(x_train, y_train.ravel())\n",
    "\n",
    "    data = {\"X_train\": x_train,\"X_test\": x_test, \"Y_train\": y_train,\"Y_test\": y_test}\n",
    "\n",
    "#     joblib.dump(data,'clean_data')\n",
    "    os.makedirs(clean_data_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"clean_data.pickle {clean_data_dir}\")\n",
    "    \n",
    "    print(os.listdir(clean_data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(clean_data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklearn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    import joblib\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "    print(clean_data_dir)\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)   \n",
    "    \n",
    "    x_train = data['X_train']\n",
    "    y_train = data['Y_train']\n",
    "\n",
    "    gbc = GradientBoostingClassifier(n_estimators=8, min_samples_split=5, max_depth=15, verbose = 1)\n",
    "    \n",
    "    # Fitting model\n",
    "    model = gbc.fit(x_train, y_train)\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(model_dir,'model.pickle'), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    print(f\"model.pickle {model_dir}\")\n",
    "    \n",
    "    print(os.listdir(model_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    clean_data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklearn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import pickle\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    print(model_dir)\n",
    "    with open(os.path.join(model_dir,'model.pickle'), 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        \n",
    "    print(model)\n",
    "    \n",
    "    print(clean_data_dir)\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "\n",
    "\n",
    "    x_test = data['X_test']\n",
    "    y_test = data['Y_test']\n",
    "\n",
    "    pred = model.predict(x_test)\n",
    "    #Model accuracy\n",
    "    accuracy = accuracy_score(y_test, pred)*100\n",
    "\n",
    "    precision = precision_score(y_test, pred)*100\n",
    "    recall = recall_score(y_test, pred)*100\n",
    "    f1 = f1_score(y_test, pred)*100\n",
    "\n",
    "\n",
    "    #ROC Score\n",
    "    roc_score = roc_auc_score(y_test, pred)*100\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion = pd.DataFrame(confusion_matrix(y_test, pred))\n",
    "\n",
    "    #ROC Score\n",
    "    fpr, tpr, threshold = roc_curve(y_test, pred)\n",
    "    roc_auc = auc(fpr, tpr)*100\n",
    "    \n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"precision\", \"numberValue\": str(precision), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"recall\", \"numberValue\": str(recall), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"f1\", \"numberValue\": str(f1), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"roc_score\", \"numberValue\": str(roc_score), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"confusion\", \"numberValue\": str(confusion), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"roc_auc\", \"numberValue\": str(roc_auc), \"format\": \"PERCENTAGE\"}\n",
    "            \n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    model_dir: InputPath(str),\n",
    "    metrics: InputPath(str),\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://minio-service.kubeflow:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\",\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, model_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}/{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_serve(\n",
    "    data_dir: str,\n",
    "    clean_data_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )()\n",
    "    \n",
    "    preprocessOp = components.func_to_container_op(preprocessing, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "    trainOp = components.func_to_container_op(train_model, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output\n",
    "    )\n",
    "\n",
    "    testOp = components.func_to_container_op(test_model, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output, trainOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        trainOp.output, testOp.output, export_bucket, model_name, model_version\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-End Loan Default\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation and export\",\n",
    ")\n",
    "def loan_default_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    clean_data_dir: str= \"/train/data\",\n",
    "    export_bucket: str = \"loan\",\n",
    "    model_name: str = \"loan\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    train_and_serve(\n",
    "        data_dir=data_dir,\n",
    "        clean_data_dir=clean_data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "    )\n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = loan_default_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End-Demo\"\n",
    "\n",
    "arguments = {\n",
    "    \"model_dir\": \"/train/model\",\n",
    "    \"data_dir\": \"/train/data\",\n",
    "    \"clean_data_dir\": \"/train/data\",\n",
    "    \"export_bucket\": \"loan\",\n",
    "    \"model_name\": \"loan\",\n",
    "    \"model_version\": \"1\",\n",
    "}\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  'loan_default.yaml')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "loan_default_pipeline",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
