{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Equipment Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Name: kfp\n",
      "Version: 1.4.0\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: UNKNOWN\n",
      "Author: google\n",
      "Author-email: None\n",
      "License: UNKNOWN\n",
      "Location: /home/jovyan/.local/lib/python3.6/site-packages\n",
      "Requires: strip-hints, kubernetes, click, kfp-pipeline-spec, requests-toolbelt, jsonschema, Deprecated, kfp-server-api, tabulate, fire, google-auth, PyYAML, cloudpickle, google-cloud-storage, docstring-parser\n",
      "Required-by: kfp-notebook\n"
     ]
    }
   ],
   "source": [
    "! pip3 show kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-20 23:29:53--  https://dl.min.io/client/mc/release/linux-amd64/mc\n",
      "Resolving dl.min.io (dl.min.io)... 178.128.69.202\n",
      "Connecting to dl.min.io (dl.min.io)|178.128.69.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20287488 (19M) [application/octet-stream]\n",
      "Saving to: ‘mc.1’\n",
      "\n",
      "mc.1                100%[===================>]  19.35M  2.24MB/s    in 74s     \n",
      "\n",
      "2021-03-20 23:31:08 (266 KB/s) - ‘mc.1’ saved [20287488/20287488]\n",
      "\n",
      "NAME:\n",
      "  mc - MinIO Client for cloud storage and filesystems.\n",
      "\n",
      "USAGE:\n",
      "  mc [FLAGS] COMMAND [COMMAND FLAGS | -h] [ARGUMENTS...]\n",
      "\n",
      "COMMANDS:\n",
      "  alias      set, remove and list aliases in configuration file\n",
      "  ls         list buckets and objects\n",
      "  mb         make a bucket\n",
      "  rb         remove a bucket\n",
      "  cp         copy objects\n",
      "  mirror     synchronize object(s) to a remote site\n",
      "  cat        display object contents\n",
      "  head       display first 'n' lines of an object\n",
      "  pipe       stream STDIN to an object\n",
      "  share      generate URL for temporary access to an object\n",
      "  find       search for objects\n",
      "  sql        run sql queries on objects\n",
      "  stat       show object metadata\n",
      "  mv         move objects\n",
      "  tree       list buckets and objects in a tree format\n",
      "  du         summarize disk usage recursively\n",
      "  retention  set retention for object(s)\n",
      "  legalhold  manage legal hold for object(s)\n",
      "  diff       list differences in object name, size, and date between two buckets\n",
      "  rm         remove objects\n",
      "  version    manage bucket versioning\n",
      "  ilm        manage bucket lifecycle\n",
      "  encrypt    manage bucket encryption config\n",
      "  event      manage object notifications\n",
      "  watch      listen for object notification events\n",
      "  undo       undo PUT/DELETE operations\n",
      "  policy     manage anonymous access to buckets and objects\n",
      "  tag        manage tags for bucket and object(s)\n",
      "  replicate  configure server side bucket replication\n",
      "  admin      manage MinIO servers\n",
      "  update     update mc to latest release\n",
      "  \n",
      "GLOBAL FLAGS:\n",
      "  --autocompletion              install auto-completion for your shell\n",
      "  --config-dir value, -C value  path to configuration folder (default: \"/home/jovyan/.mc\")\n",
      "  --quiet, -q                   disable progress bar display\n",
      "  --no-color                    disable color theme\n",
      "  --json                        enable JSON lines formatted output\n",
      "  --debug                       enable debug output\n",
      "  --insecure                    disable SSL certificate verification\n",
      "  --help, -h                    show help\n",
      "  --version, -v                 print the version\n",
      "  \n",
      "TIP:\n",
      "  Use 'mc --autocompletion' to enable shell autocompletion\n",
      "\n",
      "VERSION:\n",
      "  RELEASE.2021-03-12T03-36-59Z\n"
     ]
    }
   ],
   "source": [
    "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "! chmod +x mc\n",
    "! ./mc --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32mAdded `minio` successfully.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc mb minio/monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mRemoving `minio/monitoring/datasets.tar.gz`\u001b[0m.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc rm --recursive --force minio/monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rb minio/monitoring --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32m[2021-03-20 18:46:17 UTC]\u001b[0m\u001b[33m     0B\u001b[0m\u001b[36;1m covid/\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-03-20 23:21:46 UTC]\u001b[0m\u001b[33m     0B\u001b[0m\u001b[36;1m loan/\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-03-20 23:01:30 UTC]\u001b[0m\u001b[33m     0B\u001b[0m\u001b[36;1m mlpipeline/\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-03-20 23:31:11 UTC]\u001b[0m\u001b[33m     0B\u001b[0m\u001b[36;1m monitoring/\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! ./mc ls minio mlpipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...ts.tar.gz:  77.02 MiB / 77.02 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 13.52 MiB/s 5s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1mAccess permission for `minio/monitoring` is set to `download`\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! tar --dereference -czf datasets.tar.gz ./datasets\n",
    "! ./mc cp datasets.tar.gz minio/monitoring/datasets.tar.gz\n",
    "! ./mc policy set download minio/monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Implement Kubeflow Pipelines Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath #helps define the input & output between the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: Download the MNIST Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(data_dir: OutputPath(str)):\n",
    "\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = \"http://minio-service.kubeflow:9000/monitoring/datasets.tar.gz\"\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
    "    tar.extractall(path=data_dir)\n",
    "    \n",
    "    subprocess.call([\"ls\", \"-lha\", data_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Preprocess the Titanic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_dir: InputPath(str), clean_data_dir: OutputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklearn'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    df = pd.read_csv(f\"{data_dir}/datasets/data.csv\")\n",
    "    df = df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp','mode'], axis=1)\n",
    "    train_percentage = 0.30\n",
    "    train_size = int(len(df.index)*train_percentage)\n",
    "    x_train = df[:train_size]\n",
    "    x_test = df[train_size:490000]\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(x_train), \n",
    "                                columns=x_train.columns, \n",
    "                                index=x_train.index)\n",
    "    # Random shuffle training data\n",
    "    X_train.sample(frac=1)\n",
    "\n",
    "    X_test = pd.DataFrame(scaler.transform(x_test), \n",
    "                              columns=x_test.columns, \n",
    "                              index=x_test.index)\n",
    "    data = {\"X_train\": X_train,\"X_test\": X_test}\n",
    "    \n",
    "    \n",
    "    os.makedirs(clean_data_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"clean_data.pickle {clean_data_dir}\")\n",
    "    \n",
    "    print(os.listdir(clean_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Preprocess the Titanic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clean_data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
    "    #importing libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from keras.layers import Input, Dropout\n",
    "    from keras.layers.core import Dense \n",
    "    from keras.models import Model, Sequential, load_model\n",
    "    from keras import regularizers\n",
    "\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data) \n",
    "    \n",
    "    np.random.seed(10)\n",
    "    tf.random.set_seed(10)\n",
    "    X_train = data['X_train']\n",
    "    act_func = 'elu'\n",
    "\n",
    "    # Input layer:\n",
    "    model=Sequential()\n",
    "    # First hidden layer, connected to input vector X. \n",
    "    model.add(Dense(10,activation=act_func,\n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  kernel_regularizer=regularizers.l2(0.0),\n",
    "                  input_shape=(X_train.shape[1],)\n",
    "                )\n",
    "          )\n",
    "\n",
    "    model.add(Dense(2,activation=act_func,\n",
    "                  kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dense(10,activation=act_func,\n",
    "                  kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dense(X_train.shape[1],\n",
    "                  kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "    # Train model for 100 epochs, batch size of 10: \n",
    "    NUM_EPOCHS=5\n",
    "    BATCH_SIZE=10\n",
    "\n",
    "    model.fit(np.array(X_train),np.array(X_train),\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_split=0.1,\n",
    "                    verbose = 1)\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model.save(model_dir + \"/model.h5\")\n",
    "    print(f\"Model saved {model_dir}\")\n",
    "    print(os.listdir(model_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(clean_data_dir: InputPath(str), pca_dir: OutputPath(str)):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklean'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    import joblib\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "    \n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']  \n",
    "    print(len(X_train),len(X_test))\n",
    "    pca = PCA(n_components=2, svd_solver= 'full')\n",
    "    X_train_PCA = pca.fit_transform(X_train)\n",
    "    X_train_PCA = pd.DataFrame(X_train_PCA)\n",
    "    X_train_PCA.index = X_train.index\n",
    "\n",
    "    X_test_PCA = pca.transform(X_test)\n",
    "    X_test_PCA = pd.DataFrame(X_test_PCA)\n",
    "    X_test_PCA.index = X_test.index\n",
    "    \n",
    "    def is_pos_def(A):\n",
    "\n",
    "        if np.allclose(A, A.T):\n",
    "            try:\n",
    "                np.linalg.cholesky(A)\n",
    "                return True\n",
    "            except np.linalg.LinAlgError:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def cov_matrix(data, verbose=False):\n",
    "        covariance_matrix = np.cov(data, rowvar=False)\n",
    "        \n",
    "        if is_pos_def(covariance_matrix):\n",
    "            inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "            if is_pos_def(inv_covariance_matrix):\n",
    "                return covariance_matrix, inv_covariance_matrix\n",
    "            else:\n",
    "                print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n",
    "        else:\n",
    "            print(\"Error: Covariance Matrix is not positive definite!\")\n",
    "    def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n",
    "        inv_covariance_matrix = inv_cov_matrix\n",
    "        vars_mean = mean_distr\n",
    "        diff = data - vars_mean\n",
    "        md = []\n",
    "        for i in range(len(diff)):\n",
    "            md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n",
    "        return md\n",
    "\n",
    "    def MD_threshold(dist, extreme=False, verbose=False):\n",
    "        k = 3. if extreme else 2.\n",
    "        threshold = np.mean(dist) * k\n",
    "        return threshold\n",
    "\n",
    "    def is_pos_def(A):\n",
    "\n",
    "        if np.allclose(A, A.T):\n",
    "            try:\n",
    "                np.linalg.cholesky(A)\n",
    "                return True\n",
    "            except np.linalg.LinAlgError:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    data_train = np.array(X_train_PCA.values)\n",
    "    data_test = np.array(X_test_PCA.values)\n",
    "    cov_matrix, inv_cov_matrix  = cov_matrix(data_train)\n",
    "    mean_distr = data_train.mean(axis=0)\n",
    "    dist_test = MahalanobisDist(inv_cov_matrix, mean_distr, data_test, verbose=False)\n",
    "    dist_train = MahalanobisDist(inv_cov_matrix, mean_distr, data_train, verbose=False)\n",
    "    threshold = MD_threshold(dist_train, extreme = True)\n",
    "\n",
    "    anomaly_train = pd.DataFrame()\n",
    "    anomaly_train['Mob dist']= dist_train\n",
    "    anomaly_train['Thresh'] = threshold\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly_train['Anomaly'] = anomaly_train['Mob dist'] > anomaly_train['Thresh']\n",
    "    anomaly_train.index = X_train_PCA.index\n",
    "\n",
    "    anomaly = pd.DataFrame()\n",
    "    anomaly['Mob dist']= dist_test\n",
    "    anomaly['Thresh'] = threshold\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\n",
    "    anomaly.index = X_test_PCA.index\n",
    "\n",
    "\n",
    "    anomaly_alldata = pd.concat([anomaly_train, anomaly])\n",
    "    data = anomaly_alldata[anomaly_alldata['Anomaly']==True]\n",
    "\n",
    "    \n",
    "    os.makedirs(pca_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(pca_dir,'pca_metrics.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"pca_metrics.pickle {pca_dir}\")\n",
    "    \n",
    "    print(os.listdir(pca_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(clean_data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tensorflow import keras\n",
    "    from keras.models import load_model\n",
    "    import os\n",
    "    import joblib\n",
    "    import pickle\n",
    "    \n",
    "    print(clean_data_dir)\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "    \n",
    "    model = keras.models.load_model(model_dir + \"/model.h5\")      \n",
    "    print(model)\n",
    "    \n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    X_pred_train = model.predict(np.array(X_train))\n",
    "    X_pred_train = pd.DataFrame(X_pred_train, \n",
    "                        columns=X_train.columns)\n",
    "    X_pred_train.index = X_train.index\n",
    "\n",
    "    scored_train = pd.DataFrame(index=X_train.index)\n",
    "    scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\n",
    "    thress = np.max(scored_train['Loss_mae'])\n",
    "    scored_train['Threshold'] = thress\n",
    "    scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "    X_pred = model.predict(np.array(X_test))\n",
    "    X_pred = pd.DataFrame(X_pred, \n",
    "                        columns=X_test.columns)\n",
    "    X_pred.index = X_test.index\n",
    "\n",
    "    scored = pd.DataFrame(index=X_test.index)\n",
    "    scored['Loss_mae'] = np.mean(np.abs(X_pred-X_test), axis = 1)\n",
    "    scored['Threshold'] = thress\n",
    "    scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "    scored = pd.concat([scored_train, scored])\n",
    "    data = scored[scored['Anomaly']==True] \n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    os.makedirs(metrics_path, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(metrics_path,'metrics.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"metrics.pickle {metrics_path}\")\n",
    "    \n",
    "    print(os.listdir(metrics_path))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(pca_dir: InputPath(str), metrics_path: InputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    import pickle\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    with open(os.path.join(metrics_path,'result.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "    print(\"Autoencoder\")\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        print(f\"There are anomalies in the data, {len(data)} \\n\\n\")\n",
    "        print(data.head(20))\n",
    "    else:\n",
    "        \n",
    "        print(f\"There are no anomalies\")\n",
    "        print(\"\\n\\n **************** \\n\\n\")\n",
    "        \n",
    "    with open(os.path.join(pca_dir,'pca_metrics.pickle'), 'rb') as f:\n",
    "        data1 = pickle.load(f)\n",
    "        \n",
    "    print(data1) \n",
    "    print(\"PCA\")\n",
    "    \n",
    "    if len(data1) > 0:\n",
    "        print(f\"There are anomalies in the data, {len(data1)} \\n\\n\")\n",
    "        print(data1.head(20))\n",
    "    else:\n",
    "        print(f\"There are no anomalies\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    model_dir: InputPath(str),\n",
    "    metrics_path: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://minio-service.kubeflow:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\",\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, model_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}/{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_export(\n",
    "    data_dir: str,\n",
    "    clean_data_dir: str,\n",
    "    pca_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )()\n",
    "\n",
    "    preprocessOp = components.func_to_container_op(preprocessing, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "    \n",
    "    trainOp = components.func_to_container_op(train, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output\n",
    "    )\n",
    "    \n",
    "    pcaOp = components.func_to_container_op(PCA, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output\n",
    "    )\n",
    "\n",
    "    testOp = components.func_to_container_op(test, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output, trainOp.output\n",
    "    )\n",
    "    \n",
    "    resultOp = components.func_to_container_op(results, base_image=BASE_IMAGE)(\n",
    "        testOp.output, pcaOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        trainOp.output, testOp.output, export_bucket, model_name, model_version\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/kubeflow/kfserving/blob/master/docs/DEVELOPER_GUIDE.md#troubleshooting\n",
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-End MNIST Pipeline\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation and export\",\n",
    ")\n",
    "def monitoring_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    clean_data_dir: str = \"/train/metrics\",\n",
    "    metrics_path: str =\"/train/metrics\",\n",
    "    pca_dir: str =\"/train/data\",\n",
    "    export_bucket: str = \"monitoring\",\n",
    "    model_name: str = \"monitoring\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    train_and_export(\n",
    "        data_dir=data_dir,\n",
    "        metrics_path=metrics_path,\n",
    "        pca_dir=pca_dir,\n",
    "        clean_data_dir=clean_data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "    )\n",
    "    \n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_and_export() got an unexpected keyword argument 'metrics_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-002a4be7821a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'monitoring.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m    973\u001b[0m           \u001b[0mpipeline_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m           \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m           package_path=package_path)\n\u001b[0m\u001b[1;32m    976\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m       \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check_old_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0mpipeline_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0mparams_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         pipeline_conf)\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0m_validate_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m       \u001b[0mpipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0mpipeline_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_conf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;31m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-668a4e78377e>\u001b[0m in \u001b[0;36mmonitoring_pipeline\u001b[0;34m(model_dir, data_dir, clean_data_dir, metrics_path, pca_dir, export_bucket, model_name, model_version)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mexport_bucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_bucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmodel_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_and_export() got an unexpected keyword argument 'metrics_path'"
     ]
    }
   ],
   "source": [
    "pipeline_func = monitoring_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End-Demo\"\n",
    "\n",
    "arguments = {\n",
    "    \"model_dir\": \"/train/model\",\n",
    "    \"data_dir\": \"/train/data\",\n",
    "    \"clean_data_dir\": \"/train/data\",\n",
    "    \"model_metrics\": \"/train/metrics\",\n",
    "    \"pca_dir\": \"/train/metrics\",\n",
    "    \"export_bucket\": \"fashion_mnist\",\n",
    "    \"model_name\": \"fashion_mnist\",\n",
    "    \"model_version\": \"1\",\n",
    "}\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  'monitoring.yaml')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
