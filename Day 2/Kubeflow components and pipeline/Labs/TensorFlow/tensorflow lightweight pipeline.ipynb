{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/eb/4a3642e971f404d69d4f6fa3885559d67562801b99d7592487f1ecc4e017/pip-20.3.3-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 7.9MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.6 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-20.3.3\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.3.0.tar.gz (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Collecting docstring-parser>=0.7.3\n",
      "  Downloading docstring_parser-0.7.3.tar.gz (13 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.0\n",
      "  Downloading kfp_pipeline_spec-0.1.4-py3-none-any.whl (25 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.3.0.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.25.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.1)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting Deprecated\n",
      "  Downloading Deprecated-1.2.11-py2.py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->jsonschema>=3.0.1->kfp) (8.0.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: kfp, docstring-parser, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.3.0-py3-none-any.whl size=235905 sha256=9c5114ab4cf48bbaf6625a07a317b4fbcbd0b61cb6a22e0a08f03816ac3696f4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/56/f2/6d/72f2549e11209dc961adbfb71872b7d475f68ca97a80d2fea2\n",
      "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docstring-parser: filename=docstring_parser-0.7.3-py3-none-any.whl size=19230 sha256=1974ce704e7cfaaddec2827d6b9a69dd0daf3dd29b97248711ac9b09dbac5262\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/32/63/02/bb6eebc5261f10a6de3dcf26336a7b2b8b8dc8cacb6c00f75f\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.3.0-py3-none-any.whl size=108934 sha256=d82b544016b310baf95523a0f5e0462268fe12de4163deea2ad37e4e822ec675\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/f4/33/6a/c3e0fe65faf568ad6cc66260f187f97dd0e86b33145305c62c\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=24671 sha256=49af86c28a808336041907537c6c627af7443ab9ea6c11c53b6b41d5da69035c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/21/6d/fa/7ed7c0560e1ef39ebabd5cc0241e7fca711660bae1ad752e2b\n",
      "Successfully built kfp docstring-parser kfp-server-api strip-hints\n",
      "Installing collected packages: tabulate, strip-hints, requests-toolbelt, kfp-server-api, kfp-pipeline-spec, docstring-parser, Deprecated, click, kfp\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.11 click-7.1.2 docstring-parser-0.7.3 kfp-1.3.0 kfp-pipeline-spec-0.1.4 kfp-server-api-1.3.0 requests-toolbelt-0.9.1 strip-hints-0.1.9 tabulate-0.8.7\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after installing the necessary packages, please restart kernel before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the working directory is /home/jovyan, ensure you create the folder to save your outputs before running, \"store\" was the folder created here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the functions created are called here to ensure there are no errors before it is been truned to a component and compiled as a part of the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function that gets data from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data(data_path, working_data):\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    \n",
    "     #reading the data from its source\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\")\n",
    "    #Save the data as a pickle file to be used by the preprocess component.\n",
    "    with open(f'{data_path}/{working_data}', 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtain_data(output_dir, \"working_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path,working_data,train_data,test_data):\n",
    "    import pickle\n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    #loading the working data\n",
    "    with open(f'{data_path}/{working_data}', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    #saving the values from the dataframe\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    with open(f'{data_path}/{train_data}', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/{test_data}', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "preprocess(output_dir,\"working_data\", \"train_data\",\"test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tensorflow(data_path,train_data, model):\n",
    "    import pickle\n",
    "    # import Library\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "\n",
    "    #loading the train data\n",
    "    with open(f'{data_path}/{train_data}', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, y_train = train_data\n",
    "    \n",
    "    #initializing the classifier model with its input, hidden and output layers\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 16, activation='relu', input_dim=12,))\n",
    "    classifier.add(Dense(units = 8, activation='relu'))\n",
    "    classifier.add(Dense(units = 1, activation='sigmoid'))\n",
    "    #Compiling the classifier model with Stochastic Gradient Desecnt\n",
    "    classifier.compile(optimizer = 'adam', loss='binary_crossentropy' , metrics =['accuracy'])\n",
    "    #fitting the model\n",
    "    classifier.fit(X_train, y_train, batch_size=10, epochs=150)\n",
    "    #saving the model\n",
    "    classifier.save(f'{data_path}/{model}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples\n",
      "Epoch 1/150\n",
      "8000/8000 [==============================] - 4s 441us/sample - loss: 0.4767 - accuracy: 0.7897\n",
      "Epoch 2/150\n",
      "8000/8000 [==============================] - 3s 318us/sample - loss: 0.4095 - accuracy: 0.8242\n",
      "Epoch 3/150\n",
      "8000/8000 [==============================] - 2s 304us/sample - loss: 0.3801 - accuracy: 0.8396\n",
      "Epoch 4/150\n",
      "8000/8000 [==============================] - 2s 290us/sample - loss: 0.3600 - accuracy: 0.8510\n",
      "Epoch 5/150\n",
      "8000/8000 [==============================] - 3s 322us/sample - loss: 0.3506 - accuracy: 0.8555\n",
      "Epoch 6/150\n",
      "8000/8000 [==============================] - 2s 310us/sample - loss: 0.3453 - accuracy: 0.8577\n",
      "Epoch 7/150\n",
      "8000/8000 [==============================] - 3s 314us/sample - loss: 0.3414 - accuracy: 0.8606\n",
      "Epoch 8/150\n",
      "8000/8000 [==============================] - 3s 409us/sample - loss: 0.3398 - accuracy: 0.8610\n",
      "Epoch 9/150\n",
      "8000/8000 [==============================] - 3s 321us/sample - loss: 0.3367 - accuracy: 0.8624\n",
      "Epoch 10/150\n",
      "8000/8000 [==============================] - 3s 323us/sample - loss: 0.3360 - accuracy: 0.8643\n",
      "Epoch 11/150\n",
      "8000/8000 [==============================] - 3s 379us/sample - loss: 0.3347 - accuracy: 0.8639\n",
      "Epoch 12/150\n",
      "8000/8000 [==============================] - 3s 431us/sample - loss: 0.3337 - accuracy: 0.8641\n",
      "Epoch 13/150\n",
      "8000/8000 [==============================] - 4s 453us/sample - loss: 0.3331 - accuracy: 0.8640\n",
      "Epoch 14/150\n",
      "8000/8000 [==============================] - 3s 405us/sample - loss: 0.3316 - accuracy: 0.8627\n",
      "Epoch 15/150\n",
      "8000/8000 [==============================] - 3s 413us/sample - loss: 0.3315 - accuracy: 0.8652\n",
      "Epoch 16/150\n",
      "8000/8000 [==============================] - 4s 459us/sample - loss: 0.3313 - accuracy: 0.8644\n",
      "Epoch 17/150\n",
      "8000/8000 [==============================] - 3s 412us/sample - loss: 0.3304 - accuracy: 0.8620\n",
      "Epoch 18/150\n",
      "8000/8000 [==============================] - 3s 348us/sample - loss: 0.3303 - accuracy: 0.8643\n",
      "Epoch 19/150\n",
      "8000/8000 [==============================] - 3s 361us/sample - loss: 0.3294 - accuracy: 0.8641\n",
      "Epoch 20/150\n",
      "8000/8000 [==============================] - 3s 322us/sample - loss: 0.3294 - accuracy: 0.8639\n",
      "Epoch 21/150\n",
      "8000/8000 [==============================] - 3s 338us/sample - loss: 0.3282 - accuracy: 0.8648\n",
      "Epoch 22/150\n",
      "8000/8000 [==============================] - 3s 356us/sample - loss: 0.3290 - accuracy: 0.8637\n",
      "Epoch 23/150\n",
      "8000/8000 [==============================] - 3s 365us/sample - loss: 0.3275 - accuracy: 0.8630\n",
      "Epoch 24/150\n",
      "8000/8000 [==============================] - 3s 321us/sample - loss: 0.3282 - accuracy: 0.8649\n",
      "Epoch 25/150\n",
      "8000/8000 [==============================] - 3s 343us/sample - loss: 0.3276 - accuracy: 0.8656\n",
      "Epoch 26/150\n",
      "8000/8000 [==============================] - 3s 328us/sample - loss: 0.3265 - accuracy: 0.8666\n",
      "Epoch 27/150\n",
      "8000/8000 [==============================] - 3s 380us/sample - loss: 0.3275 - accuracy: 0.8644\n",
      "Epoch 28/150\n",
      "8000/8000 [==============================] - 3s 343us/sample - loss: 0.3264 - accuracy: 0.8645\n",
      "Epoch 29/150\n",
      "8000/8000 [==============================] - 3s 364us/sample - loss: 0.3265 - accuracy: 0.8652\n",
      "Epoch 30/150\n",
      "8000/8000 [==============================] - 3s 346us/sample - loss: 0.3260 - accuracy: 0.8640\n",
      "Epoch 31/150\n",
      "8000/8000 [==============================] - 3s 339us/sample - loss: 0.3264 - accuracy: 0.8631\n",
      "Epoch 32/150\n",
      "8000/8000 [==============================] - 3s 353us/sample - loss: 0.3264 - accuracy: 0.8654\n",
      "Epoch 33/150\n",
      "8000/8000 [==============================] - 3s 390us/sample - loss: 0.3254 - accuracy: 0.8671\n",
      "Epoch 34/150\n",
      "8000/8000 [==============================] - 4s 494us/sample - loss: 0.3261 - accuracy: 0.8641\n",
      "Epoch 35/150\n",
      "8000/8000 [==============================] - 3s 412us/sample - loss: 0.3256 - accuracy: 0.8626\n",
      "Epoch 36/150\n",
      "8000/8000 [==============================] - 3s 367us/sample - loss: 0.3256 - accuracy: 0.8655\n",
      "Epoch 37/150\n",
      "8000/8000 [==============================] - 3s 354us/sample - loss: 0.3263 - accuracy: 0.8658\n",
      "Epoch 38/150\n",
      "8000/8000 [==============================] - 3s 366us/sample - loss: 0.3249 - accuracy: 0.8652\n",
      "Epoch 39/150\n",
      "8000/8000 [==============================] - 3s 371us/sample - loss: 0.3244 - accuracy: 0.8655\n",
      "Epoch 40/150\n",
      "8000/8000 [==============================] - 3s 359us/sample - loss: 0.3255 - accuracy: 0.8645\n",
      "Epoch 41/150\n",
      "8000/8000 [==============================] - 3s 329us/sample - loss: 0.3240 - accuracy: 0.8650\n",
      "Epoch 42/150\n",
      "8000/8000 [==============================] - 3s 346us/sample - loss: 0.3250 - accuracy: 0.8633\n",
      "Epoch 43/150\n",
      "8000/8000 [==============================] - 3s 370us/sample - loss: 0.3238 - accuracy: 0.8640\n",
      "Epoch 44/150\n",
      "8000/8000 [==============================] - 3s 337us/sample - loss: 0.3237 - accuracy: 0.8649\n",
      "Epoch 45/150\n",
      "8000/8000 [==============================] - 3s 344us/sample - loss: 0.3241 - accuracy: 0.8655\n",
      "Epoch 46/150\n",
      "8000/8000 [==============================] - 3s 377us/sample - loss: 0.3235 - accuracy: 0.8641\n",
      "Epoch 47/150\n",
      "8000/8000 [==============================] - 3s 358us/sample - loss: 0.3238 - accuracy: 0.8659\n",
      "Epoch 48/150\n",
      "8000/8000 [==============================] - 3s 337us/sample - loss: 0.3243 - accuracy: 0.8666\n",
      "Epoch 49/150\n",
      "8000/8000 [==============================] - 3s 338us/sample - loss: 0.3241 - accuracy: 0.8656\n",
      "Epoch 50/150\n",
      "8000/8000 [==============================] - 3s 364us/sample - loss: 0.3245 - accuracy: 0.8664\n",
      "Epoch 51/150\n",
      "8000/8000 [==============================] - 3s 340us/sample - loss: 0.3233 - accuracy: 0.8649\n",
      "Epoch 52/150\n",
      "8000/8000 [==============================] - 3s 313us/sample - loss: 0.3237 - accuracy: 0.8652\n",
      "Epoch 53/150\n",
      "8000/8000 [==============================] - 2s 307us/sample - loss: 0.3234 - accuracy: 0.8626\n",
      "Epoch 54/150\n",
      "8000/8000 [==============================] - 3s 345us/sample - loss: 0.3232 - accuracy: 0.8654\n",
      "Epoch 55/150\n",
      "8000/8000 [==============================] - 2s 295us/sample - loss: 0.3230 - accuracy: 0.8665\n",
      "Epoch 56/150\n",
      "8000/8000 [==============================] - 2s 275us/sample - loss: 0.3225 - accuracy: 0.8652\n",
      "Epoch 57/150\n",
      "8000/8000 [==============================] - 2s 294us/sample - loss: 0.3231 - accuracy: 0.8654\n",
      "Epoch 58/150\n",
      "8000/8000 [==============================] - 2s 279us/sample - loss: 0.3228 - accuracy: 0.8662\n",
      "Epoch 59/150\n",
      "8000/8000 [==============================] - 2s 266us/sample - loss: 0.3232 - accuracy: 0.8648\n",
      "Epoch 60/150\n",
      "8000/8000 [==============================] - 2s 255us/sample - loss: 0.3215 - accuracy: 0.8649\n",
      "Epoch 61/150\n",
      "8000/8000 [==============================] - 2s 269us/sample - loss: 0.3221 - accuracy: 0.8650\n",
      "Epoch 62/150\n",
      "8000/8000 [==============================] - 2s 270us/sample - loss: 0.3212 - accuracy: 0.8656\n",
      "Epoch 63/150\n",
      "8000/8000 [==============================] - 2s 277us/sample - loss: 0.3219 - accuracy: 0.8658\n",
      "Epoch 64/150\n",
      "8000/8000 [==============================] - 2s 272us/sample - loss: 0.3224 - accuracy: 0.8656\n",
      "Epoch 65/150\n",
      "8000/8000 [==============================] - 2s 254us/sample - loss: 0.3211 - accuracy: 0.8652\n",
      "Epoch 66/150\n",
      "8000/8000 [==============================] - 3s 351us/sample - loss: 0.3221 - accuracy: 0.8662\n",
      "Epoch 67/150\n",
      "8000/8000 [==============================] - 2s 274us/sample - loss: 0.3218 - accuracy: 0.8679\n",
      "Epoch 68/150\n",
      "8000/8000 [==============================] - 2s 254us/sample - loss: 0.3208 - accuracy: 0.8643\n",
      "Epoch 69/150\n",
      "8000/8000 [==============================] - 2s 243us/sample - loss: 0.3210 - accuracy: 0.8669\n",
      "Epoch 70/150\n",
      "8000/8000 [==============================] - 2s 243us/sample - loss: 0.3218 - accuracy: 0.8658\n",
      "Epoch 71/150\n",
      "8000/8000 [==============================] - 2s 275us/sample - loss: 0.3208 - accuracy: 0.8669\n",
      "Epoch 72/150\n",
      "8000/8000 [==============================] - 2s 276us/sample - loss: 0.3207 - accuracy: 0.8656\n",
      "Epoch 73/150\n",
      "8000/8000 [==============================] - 2s 269us/sample - loss: 0.3206 - accuracy: 0.8662\n",
      "Epoch 74/150\n",
      "8000/8000 [==============================] - 2s 262us/sample - loss: 0.3201 - accuracy: 0.8698\n",
      "Epoch 75/150\n",
      "8000/8000 [==============================] - 2s 259us/sample - loss: 0.3201 - accuracy: 0.8660\n",
      "Epoch 76/150\n",
      "8000/8000 [==============================] - 2s 269us/sample - loss: 0.3199 - accuracy: 0.8664\n",
      "Epoch 77/150\n",
      "8000/8000 [==============================] - 2s 268us/sample - loss: 0.3198 - accuracy: 0.8665\n",
      "Epoch 78/150\n",
      "8000/8000 [==============================] - 2s 266us/sample - loss: 0.3195 - accuracy: 0.8668\n",
      "Epoch 79/150\n",
      "8000/8000 [==============================] - 2s 248us/sample - loss: 0.3198 - accuracy: 0.8670\n",
      "Epoch 80/150\n",
      "8000/8000 [==============================] - 2s 267us/sample - loss: 0.3204 - accuracy: 0.8659\n",
      "Epoch 81/150\n",
      "8000/8000 [==============================] - 2s 272us/sample - loss: 0.3202 - accuracy: 0.8679\n",
      "Epoch 82/150\n",
      "8000/8000 [==============================] - 2s 268us/sample - loss: 0.3200 - accuracy: 0.8680\n",
      "Epoch 83/150\n",
      "8000/8000 [==============================] - 2s 254us/sample - loss: 0.3192 - accuracy: 0.8664\n",
      "Epoch 84/150\n",
      "8000/8000 [==============================] - 2s 249us/sample - loss: 0.3198 - accuracy: 0.8668\n",
      "Epoch 85/150\n",
      "8000/8000 [==============================] - 2s 263us/sample - loss: 0.3188 - accuracy: 0.8694\n",
      "Epoch 86/150\n",
      "8000/8000 [==============================] - 2s 275us/sample - loss: 0.3200 - accuracy: 0.8652\n",
      "Epoch 87/150\n",
      "8000/8000 [==============================] - 2s 268us/sample - loss: 0.3194 - accuracy: 0.8676\n",
      "Epoch 88/150\n",
      "8000/8000 [==============================] - 2s 260us/sample - loss: 0.3192 - accuracy: 0.8681\n",
      "Epoch 89/150\n",
      "8000/8000 [==============================] - 2s 265us/sample - loss: 0.3195 - accuracy: 0.8677\n",
      "Epoch 90/150\n",
      "8000/8000 [==============================] - 2s 261us/sample - loss: 0.3197 - accuracy: 0.8675\n",
      "Epoch 91/150\n",
      "8000/8000 [==============================] - 2s 274us/sample - loss: 0.3188 - accuracy: 0.8673\n",
      "Epoch 92/150\n",
      "8000/8000 [==============================] - 2s 264us/sample - loss: 0.3190 - accuracy: 0.8674\n",
      "Epoch 93/150\n",
      "8000/8000 [==============================] - 2s 254us/sample - loss: 0.3196 - accuracy: 0.8666\n",
      "Epoch 94/150\n",
      "8000/8000 [==============================] - 2s 258us/sample - loss: 0.3183 - accuracy: 0.8677\n",
      "Epoch 95/150\n",
      "8000/8000 [==============================] - 2s 274us/sample - loss: 0.3178 - accuracy: 0.8686\n",
      "Epoch 96/150\n",
      "8000/8000 [==============================] - 2s 271us/sample - loss: 0.3184 - accuracy: 0.8701\n",
      "Epoch 97/150\n",
      "8000/8000 [==============================] - 2s 260us/sample - loss: 0.3183 - accuracy: 0.8677\n",
      "Epoch 98/150\n",
      "8000/8000 [==============================] - 2s 249us/sample - loss: 0.3188 - accuracy: 0.8668\n",
      "Epoch 99/150\n",
      "8000/8000 [==============================] - 2s 236us/sample - loss: 0.3191 - accuracy: 0.8684\n",
      "Epoch 100/150\n",
      "8000/8000 [==============================] - 2s 235us/sample - loss: 0.3187 - accuracy: 0.8681\n",
      "Epoch 101/150\n",
      "8000/8000 [==============================] - 2s 253us/sample - loss: 0.3180 - accuracy: 0.8681\n",
      "Epoch 102/150\n",
      "8000/8000 [==============================] - 2s 235us/sample - loss: 0.3191 - accuracy: 0.8671\n",
      "Epoch 103/150\n",
      "8000/8000 [==============================] - 2s 227us/sample - loss: 0.3188 - accuracy: 0.8668\n",
      "Epoch 104/150\n",
      "8000/8000 [==============================] - 2s 234us/sample - loss: 0.3184 - accuracy: 0.8666\n",
      "Epoch 105/150\n",
      "8000/8000 [==============================] - 2s 236us/sample - loss: 0.3184 - accuracy: 0.8669\n",
      "Epoch 106/150\n",
      "8000/8000 [==============================] - 2s 241us/sample - loss: 0.3176 - accuracy: 0.8685\n",
      "Epoch 107/150\n",
      "8000/8000 [==============================] - 2s 243us/sample - loss: 0.3182 - accuracy: 0.8666\n",
      "Epoch 108/150\n",
      "8000/8000 [==============================] - 2s 225us/sample - loss: 0.3170 - accuracy: 0.8681\n",
      "Epoch 109/150\n",
      "8000/8000 [==============================] - 2s 246us/sample - loss: 0.3181 - accuracy: 0.8666\n",
      "Epoch 110/150\n",
      "8000/8000 [==============================] - 2s 243us/sample - loss: 0.3177 - accuracy: 0.8673\n",
      "Epoch 111/150\n",
      "8000/8000 [==============================] - 2s 265us/sample - loss: 0.3175 - accuracy: 0.8677\n",
      "Epoch 112/150\n",
      "8000/8000 [==============================] - 2s 251us/sample - loss: 0.3175 - accuracy: 0.8679\n",
      "Epoch 113/150\n",
      "8000/8000 [==============================] - 2s 217us/sample - loss: 0.3178 - accuracy: 0.8685\n",
      "Epoch 114/150\n",
      "8000/8000 [==============================] - 2s 230us/sample - loss: 0.3171 - accuracy: 0.8674\n",
      "Epoch 115/150\n",
      "8000/8000 [==============================] - 2s 239us/sample - loss: 0.3179 - accuracy: 0.8658\n",
      "Epoch 116/150\n",
      "8000/8000 [==============================] - 2s 238us/sample - loss: 0.3176 - accuracy: 0.8658\n",
      "Epoch 117/150\n",
      "8000/8000 [==============================] - 2s 236us/sample - loss: 0.3170 - accuracy: 0.8684\n",
      "Epoch 118/150\n",
      "8000/8000 [==============================] - 2s 236us/sample - loss: 0.3174 - accuracy: 0.8665\n",
      "Epoch 119/150\n",
      "8000/8000 [==============================] - 2s 249us/sample - loss: 0.3174 - accuracy: 0.8681\n",
      "Epoch 120/150\n",
      "8000/8000 [==============================] - 2s 240us/sample - loss: 0.3164 - accuracy: 0.8702\n",
      "Epoch 121/150\n",
      "8000/8000 [==============================] - 2s 235us/sample - loss: 0.3162 - accuracy: 0.8674\n",
      "Epoch 122/150\n",
      "8000/8000 [==============================] - 2s 238us/sample - loss: 0.3160 - accuracy: 0.8671\n",
      "Epoch 123/150\n",
      "8000/8000 [==============================] - 2s 233us/sample - loss: 0.3167 - accuracy: 0.8669\n",
      "Epoch 124/150\n",
      "8000/8000 [==============================] - 2s 208us/sample - loss: 0.3153 - accuracy: 0.8666\n",
      "Epoch 125/150\n",
      "8000/8000 [==============================] - 2s 234us/sample - loss: 0.3168 - accuracy: 0.8677\n",
      "Epoch 126/150\n",
      "8000/8000 [==============================] - 2s 246us/sample - loss: 0.3160 - accuracy: 0.8687\n",
      "Epoch 127/150\n",
      "8000/8000 [==============================] - 2s 255us/sample - loss: 0.3152 - accuracy: 0.8674\n",
      "Epoch 128/150\n",
      "8000/8000 [==============================] - 2s 241us/sample - loss: 0.3168 - accuracy: 0.8673\n",
      "Epoch 129/150\n",
      "8000/8000 [==============================] - 2s 225us/sample - loss: 0.3160 - accuracy: 0.8661\n",
      "Epoch 130/150\n",
      "8000/8000 [==============================] - 2s 228us/sample - loss: 0.3166 - accuracy: 0.8665\n",
      "Epoch 131/150\n",
      "8000/8000 [==============================] - 2s 253us/sample - loss: 0.3158 - accuracy: 0.8666\n",
      "Epoch 132/150\n",
      "8000/8000 [==============================] - 2s 237us/sample - loss: 0.3157 - accuracy: 0.8679\n",
      "Epoch 133/150\n",
      "8000/8000 [==============================] - 2s 237us/sample - loss: 0.3162 - accuracy: 0.8665\n",
      "Epoch 134/150\n",
      "8000/8000 [==============================] - 2s 224us/sample - loss: 0.3157 - accuracy: 0.8686\n",
      "Epoch 135/150\n",
      "8000/8000 [==============================] - 2s 230us/sample - loss: 0.3152 - accuracy: 0.8677\n",
      "Epoch 136/150\n",
      "8000/8000 [==============================] - 2s 234us/sample - loss: 0.3145 - accuracy: 0.8686\n",
      "Epoch 137/150\n",
      "8000/8000 [==============================] - 2s 243us/sample - loss: 0.3146 - accuracy: 0.8676\n",
      "Epoch 138/150\n",
      "8000/8000 [==============================] - 2s 218us/sample - loss: 0.3156 - accuracy: 0.8690\n",
      "Epoch 139/150\n",
      "8000/8000 [==============================] - 2s 234us/sample - loss: 0.3146 - accuracy: 0.8675\n",
      "Epoch 140/150\n",
      "8000/8000 [==============================] - 2s 215us/sample - loss: 0.3142 - accuracy: 0.8675\n",
      "Epoch 141/150\n",
      "8000/8000 [==============================] - 2s 233us/sample - loss: 0.3151 - accuracy: 0.8681\n",
      "Epoch 142/150\n",
      "8000/8000 [==============================] - 2s 258us/sample - loss: 0.3153 - accuracy: 0.8665\n",
      "Epoch 143/150\n",
      "8000/8000 [==============================] - 2s 272us/sample - loss: 0.3152 - accuracy: 0.8696\n",
      "Epoch 144/150\n",
      "8000/8000 [==============================] - 2s 237us/sample - loss: 0.3150 - accuracy: 0.8661\n",
      "Epoch 145/150\n",
      "8000/8000 [==============================] - 2s 221us/sample - loss: 0.3149 - accuracy: 0.8673\n",
      "Epoch 146/150\n",
      "8000/8000 [==============================] - 2s 231us/sample - loss: 0.3149 - accuracy: 0.8687\n",
      "Epoch 147/150\n",
      "8000/8000 [==============================] - 2s 242us/sample - loss: 0.3143 - accuracy: 0.8687\n",
      "Epoch 148/150\n",
      "8000/8000 [==============================] - 2s 271us/sample - loss: 0.3146 - accuracy: 0.8686\n",
      "Epoch 149/150\n",
      "8000/8000 [==============================] - 2s 247us/sample - loss: 0.3153 - accuracy: 0.8676\n",
      "Epoch 150/150\n",
      "8000/8000 [==============================] - 2s 234us/sample - loss: 0.3144 - accuracy: 0.8683\n",
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/jovyan/store/model/assets\n"
     ]
    }
   ],
   "source": [
    "train_tensorflow(output_dir, \"train_data\",\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tensorflow(data_path,test_data,model):\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import load_model\n",
    "    \n",
    "    #loading the X_test and y_test\n",
    "    with open(f'{data_path}/{test_data}', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the X_test from y_test.\n",
    "    X_test, y_test = test_data\n",
    "    #loading the model\n",
    "    classifier = load_model(f'{data_path}/{model}')\n",
    "\n",
    "    #Evaluate the model and print the results\n",
    "    test_loss, test_acc = classifier.evaluate(X_test,  y_test, verbose=0)\n",
    "    \n",
    "    #model's prediction on test data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    # create a threshold for the confution matrics\n",
    "    y_pred=(y_pred>0.5)\n",
    "\n",
    "    #saving the test_loss and test_acc\n",
    "    with open(f'{data_path}/performance.txt', 'w') as f:\n",
    "        f.write(\"Test_loss: {}, Test_accuracy: {} \".format(test_loss,test_acc))\n",
    "\n",
    "    #saving the predictions\n",
    "    with open(f'{data_path}/results.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {}, Actual: {} \".format(y_pred,y_test.astype(np.bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tensorflow(output_dir,\"test_data\", \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the components from the python functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create light weight components\n",
    "obtain_data_op = kfp.components.create_component_from_func(obtain_data,base_image=\"python:3.7.1\")\n",
    "preprocess_op = kfp.components.create_component_from_func(preprocess,base_image=\"python:3.7.1\")\n",
    "train_op = kfp.components.create_component_from_func(train_tensorflow, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = kfp.components.create_component_from_func(predict_tensorflow, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client that would enable communication with the Pipelines API server \n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"Churn Pipeline\", description=\"Performs Preprocessing, training and prediction of churn rate\")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def churn_lightweight_tensorflow_pipeline(data_path: str,\n",
    "                                          working_data: str,\n",
    "                                         train_data: str,\n",
    "                                         test_data:str,\n",
    "                                         model:str):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    volume_op = dsl.VolumeOp(\n",
    "    name=\"data_volume\",\n",
    "    resource_name=\"data-volume\",\n",
    "    size=\"1Gi\",\n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    #create obtain data component\n",
    "    obtain_data_container = obtain_data_op(data_path, working_data).add_pvolumes({data_path: volume_op.volume})\n",
    "    # Create preprocess components.\n",
    "    preprocess_container = preprocess_op(data_path, working_data, train_data, test_data).add_pvolumes({data_path: obtain_data_container.pvolume})\n",
    "    # Create train component.\n",
    "    train_container = train_op(data_path, train_data, model).add_pvolumes({data_path: preprocess_container.pvolume})\n",
    "    # Create prediction component.\n",
    "    predict_container = predict_op(data_path, test_data, model).add_pvolumes({data_path: train_container.pvolume})\n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: predict_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/results.txt']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/dsl/_container_op.py:1039: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/9dab74bd-f7e6-47d5-be51-094fd0f364b3\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/c59c1407-4ce2-4749-b388-280beecd8e30\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "DATA = \"working_data\"\n",
    "TRAIN_DATA = \"train_data\"\n",
    "TEST_DATA = \"test_data\"\n",
    "MODEL_FILE= \"classifier.h5\"\n",
    "\n",
    "\n",
    "pipeline_func = churn_lightweight_tensorflow_pipeline\n",
    "\n",
    "experiment_name = 'churn_prediction_tensorflow_lightweight'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"working_data\": DATA,\n",
    "            \"train_data\": TRAIN_DATA,\n",
    "            \"test_data\": TEST_DATA,\n",
    "            \"model\":MODEL_FILE}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
